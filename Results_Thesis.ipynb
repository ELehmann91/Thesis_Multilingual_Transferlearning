{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Results_Thesis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cjj7J65SZxsE",
        "RffXXZigrgGL",
        "eszN0r_mWfF9"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ELehmann91/Thesis_Multilingual_Transferlearning/blob/master/Results_Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru35-Xo1kmrW",
        "colab_type": "text"
      },
      "source": [
        "# Train script\n",
        "  \n",
        "using prewritten fuctions and standardized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5sB703QrHaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7967cfd7-a3dc-4c6a-92b2-ca76a4e36931"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path ='/content/gdrive/My Drive/Thesis_ecb_ecoicop'"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJUAJfPTnMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install eli5\n",
        "!git clone 'https://github.com/ELehmann91/Thesis_Multilingual_Transferlearning'\n",
        "\n",
        "%cd Thesis_Multilingual_Transferlearning\n",
        "import labeler_cc5\n",
        "import coicop_model\n",
        "import model_helper\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iupya9-dk4m0",
        "colab_type": "text"
      },
      "source": [
        "## Import Data\n",
        "  \n",
        "data in the normalized folder are splitted between languages (currently de fr & it) and share the same columns so they can be merged easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvvxjN1mrrZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2cab12a6-f5d7-4c95-cc60-2faf12207d32"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_path = '/data/'#\n",
        "#file_path = 'fra/carrfour_trans_pred.csv'\n",
        "file_path = 'normalized/norm_fr.csv'\n",
        "file_path2 = 'normalized/norm_de.csv'\n",
        "file_path3 = 'at/norm_at.csv'\n",
        "#file_path = 'edeka_pred.csv'\n",
        "\n",
        "# french data\n",
        "df_fr = pd.read_csv(path+data_path+file_path,sep='|',index_col=False)\n",
        "\n",
        "# only already labeled\n",
        "df_fr = df_fr[df_fr['cc5'].isna()==False]\n",
        "df_fr = df_fr[df_fr['shop'].isin( ['carrefour','auchan'])]#,'banque_de_france'])]\n",
        "df_fr['cc5'] = df_fr['cc5'].apply(lambda x: '9999_Non-Food' if int(str(x)[0])>2 else x)\n",
        "print(len(df_fr))\n",
        "\n",
        "# german data\n",
        "df_de = pd.read_csv(path+data_path+file_path2,sep='|',index_col=False)\n",
        "# only already labeled\n",
        "df_de = df_de[df_de['cc5'].isna()==False]\n",
        "print(len(df_de))\n",
        "# austrian data\n",
        "df_at = pd.read_csv(path+data_path+file_path3,sep='|',index_col=False)\n",
        "# only already labeled\n",
        "df_at = df_at[df_at['cc5'].isna()==False]\n",
        "df_de = df_de.append(df_at)\n",
        "print(len(df_de))\n",
        "\n",
        "df_fr = df_fr.sample(frac=1).reset_index(drop=True)\n",
        "df_de = df_de.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7923\n",
            "21903\n",
            "23199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZH6sOjr2_no",
        "colab_type": "text"
      },
      "source": [
        "### Exclude non-food?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-40uLS9OAZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "11711a4c-9445-452d-deda-328596a7aeef"
      },
      "source": [
        "no_Classes = 75\n",
        "exclude_non_food = True\n",
        "if exclude_non_food:\n",
        "    df_fr = df_fr[df_fr['cc5']!='9999_Non-Food']\n",
        "    df_de = df_de[df_de['cc5']!='9999_Non-Food']\n",
        "    df_at = df_at[df_at['cc5']!='9999_Non-Food']\n",
        "    no_Classes = 74\n",
        "    print(len(df_fr))\n",
        "    print(len(df_de))\n",
        "    print(len(df_at))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7225\n",
            "22064\n",
            "241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vswr191ClXi1",
        "colab_type": "text"
      },
      "source": [
        "decide which columns should be used for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n55A0HV-g95g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var = 'name'\n",
        "#df_fr['text'] = df_fr[var].fillna('unknown') \n",
        "#df_de['text'] = df_de[var].fillna('unknown') \n",
        "#df_at['text'] = df_at[var].fillna('unknown') \n",
        "\n",
        "var1 = 'categ'\n",
        "var2 = 'words_from_url'\n",
        "#df_fr['text'] = df_fr[var1].fillna('unknown')  + ' <sep> ' + df_fr[var2].fillna('unknown') \n",
        "#df_de['text'] = df_de[var1].fillna('unknown')  + ' <sep> ' + df_de[var2].fillna('unknown') \n",
        "#df_at['text'] = df_at[var1].fillna('unknown')  + ' <sep> ' + df_at[var2].fillna('unknown') \n",
        "\n",
        "\n",
        "var1 = 'categ'\n",
        "var2 = 'words_from_url' \n",
        "var3 = 'name'\n",
        "df_fr['text'] = ' <fr> ' + df_fr[var1].fillna('unknown')  + ' <sep> ' + df_fr[var2].fillna('unknown')  + ' <sep> ' + df_fr[var3].fillna('unknown') \n",
        "df_de['text'] = ' <de> ' + df_de[var1].fillna('unknown')  + ' <sep> ' + df_de[var2].fillna('unknown')  + ' <sep> ' + df_de[var3].fillna('unknown') \n",
        "df_at['text'] = ' <de> ' + df_at[var1].fillna('unknown')  + ' <sep> ' + df_at[var2].fillna('unknown')  + ' <sep> ' + df_at[var3].fillna('unknown') \n",
        "\n",
        "#df_fr['text'] = df_fr[var1].fillna('unknown')  + df_fr[var2].fillna('unknown')  + df_fr[var3].fillna('unknown') \n",
        "#df_de['text'] = df_de[var1].fillna('unknown')  + df_de[var2].fillna('unknown')  + df_de[var3].fillna('unknown') \n",
        "#df_at['text'] = df_at[var1].fillna('unknown')  + df_at[var2].fillna('unknown')  + df_at[var3].fillna('unknown') \n",
        "\n",
        "#df_fr['text'] = ' <fr> ' + df_fr['name'] + ' <sep> ' + df_fr['categ'].fillna('unknown') + ' <sep> ' + df_fr['words_from_url'].fillna('unknown') + ' <sep> ' + df_fr['prod_desc'].fillna('unknown') \n",
        "#df_de['text'] = ' <de> ' + df_de['name'] + ' <sep> ' + df_de['categ'].fillna('unknown') + ' <sep> ' + df_de['words_from_url'].fillna('unknown') + ' <sep> ' + df_de['prod_desc'].fillna('unknown') \n",
        "#df_at['text'] = ' <de> ' + df_at['name'] + ' <sep> ' + df_at['categ'].fillna('unknown') + ' <sep> ' + df_at['words_from_url'].fillna('unknown') + ' <sep> ' + df_at['prod_desc'].fillna('unknown') \n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScESyOiJNwoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "rep_dict = {'.':' ',\n",
        "                        ',': ' ',\n",
        "                        '&': ' ',\n",
        "                        '-': ' ',\n",
        "                        '/': ' ',\n",
        "                        'ü': 'ue',\n",
        "                        'ä': 'ae',\n",
        "                        'ö': 'oe',\n",
        "                        'ß': 'ss',\n",
        "                        'ê': 'e',\n",
        "                        'é': 'e',\n",
        "                        'è': 'e',\n",
        "                        'â': 'a',\n",
        "                        'á': 'a',\n",
        "                        'à': 'a',\n",
        "                        'ô':'o',\n",
        "                        'œ': 'ae',\n",
        "                        '%': ' percent ',\n",
        "                        '1': ' one ',\n",
        "                        '2': ' two ',\n",
        "                        '3': ' three ',\n",
        "                        '4': ' four ',\n",
        "                        '5': ' five ',\n",
        "                        '6': ' six ',\n",
        "                        '7': ' seven ',\n",
        "                        '8': ' eigth ',\n",
        "                        '9': ' nine ',\n",
        "                        '0': ' zero ',\n",
        "                        ' l ':' liter ',\n",
        "                        ' ml ':' liter '\n",
        "                        }\n",
        "\n",
        "def prepro(line):\n",
        "    if isinstance(line,str):\n",
        "        text_str = ' '.join(str(t) for t in line.split())\n",
        "        text_str = text_str.lower()\n",
        "        for a,b in rep_dict.items():\n",
        "            text_str = text_str.replace(a,b)\n",
        "        text_str = re.sub('[^a-zäöüàáâéèêßœ<>]+', ' ', text_str)\n",
        "    else: \n",
        "        text_str = str(line)\n",
        "    return text_str"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1on_by8aNzmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fr['text'] = df_fr['text'].apply(lambda x:prepro(x))\n",
        "df_de['text'] = df_de['text'].apply(lambda x:prepro(x))\n",
        "df_at['text'] = df_at['text'].apply(lambda x:prepro(x))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUiw5fuU250I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d46671f6-d384-47a2-bcb1-3949936145dc"
      },
      "source": [
        "print('50% quantile no. of words per row french',np.quantile(df_fr['text'].apply(lambda x: len(str(x).split())),.50))\n",
        "print('50% quantile no. of words per row german',np.quantile(df_de['text'].apply(lambda x: len(str(x).split())),.50))\n",
        "print('50% quantile no. of words per row austrian',np.quantile(df_at['text'].apply(lambda x: len(str(x).split())),.50))\n",
        "\n",
        "print('90% quantile no. of words per row french',np.quantile(df_fr['text'].apply(lambda x: len(str(x).split())),.90))\n",
        "print('90% quantile no. of words per row german',np.quantile(df_de['text'].apply(lambda x: len(str(x).split())),.90))\n",
        "print('90% quantile no. of words per row austrian',np.quantile(df_at['text'].apply(lambda x: len(str(x).split())),.90))\n",
        "\n",
        "seq_len = int(max(np.quantile(df_fr['text'].apply(lambda x: len(str(x).split())),.95),np.quantile(df_de['text'].apply(lambda x: len(str(x).split())),.95)))\n",
        "print('seq_len',seq_len)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50% quantile no. of words per row french 28.0\n",
            "50% quantile no. of words per row german 19.0\n",
            "50% quantile no. of words per row austrian 19.0\n",
            "90% quantile no. of words per row french 36.0\n",
            "90% quantile no. of words per row german 29.700000000000728\n",
            "90% quantile no. of words per row austrian 24.0\n",
            "seq_len 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPbed6wD3pXB",
        "colab_type": "text"
      },
      "source": [
        "### Split Train Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvZmopEj259h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99c2b729-824a-4470-9f89-0132683482fb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_train_abs(df):\n",
        "    X_train, X_val_test, y_train, y_val_test  = train_test_split(df['text'], df['cc5'], random_state=42, shuffle=True)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val_test , y_val_test, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "X_train_de, X_val_de, X_test_de, y_train_de, y_val_de, y_test_de = split_train_abs(df_de)\n",
        "X_train_fr, X_val_fr, X_test_fr, y_train_fr, y_val_fr, y_test_fr = split_train_abs(df_fr)\n",
        "\n",
        "print('de',X_train_de.shape, X_val_de.shape, X_test_de.shape, y_train_de.shape, y_val_de.shape, y_test_de.shape )\n",
        "print('fr',X_train_fr.shape, X_val_fr.shape, X_test_fr.shape, y_train_fr.shape, y_val_fr.shape, y_test_fr.shape )"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "de (16548,) (4137,) (1379,) (16548,) (4137,) (1379,)\n",
            "fr (5418,) (1355,) (452,) (5418,) (1355,) (452,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5t7nvAB6U7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e17e6a2-ed70-4723-de8f-b926b25b82c1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer_de = Tokenizer()\n",
        "tokenizer_de.fit_on_texts(X_train_de.append(X_val_de))\n",
        "vocab_size_de = len(tokenizer_de.word_index) + 1\n",
        "\n",
        "tokenizer_fr = Tokenizer()\n",
        "tokenizer_fr.fit_on_texts(X_train_fr.append(X_val_fr))\n",
        "vocab_size_fr = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "tokenizer_de_fr = Tokenizer()\n",
        "tokenizer_de_fr.fit_on_texts(X_train_de.append(X_val_de).append(X_train_fr).append(X_val_fr))\n",
        "\n",
        "vocab_size_de_fr = len(tokenizer_de_fr.word_index) + 1\n",
        "print(vocab_size_de,vocab_size_fr,vocab_size_de_fr)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12590 3991 15261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBPdY7e98jRw",
        "colab_type": "text"
      },
      "source": [
        "tokenize & pad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYz66JnJ5dNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#german\n",
        "X_train_tokens_de = tokenizer_de.texts_to_sequences(X_train_de)\n",
        "X_val_tokens_de = tokenizer_de.texts_to_sequences(X_val_de)\n",
        "X_test_tokens_de = tokenizer_de.texts_to_sequences(X_test_de)\n",
        "\n",
        "X_train_pad_de = pad_sequences(X_train_tokens_de,maxlen=seq_len, padding='post')\n",
        "X_val_pad_de = pad_sequences(X_val_tokens_de,maxlen=seq_len, padding='post')\n",
        "X_test_pad_de = pad_sequences(X_test_tokens_de,maxlen=seq_len, padding='post')\n",
        "\n",
        "#french\n",
        "X_train_tokens_fr = tokenizer_fr.texts_to_sequences(X_train_fr)\n",
        "X_val_tokens_fr = tokenizer_fr.texts_to_sequences(X_val_fr)\n",
        "X_test_tokens_fr = tokenizer_fr.texts_to_sequences(X_test_fr)\n",
        "\n",
        "X_train_pad_fr = pad_sequences(X_train_tokens_fr,maxlen=seq_len, padding='post')\n",
        "X_val_pad_fr = pad_sequences(X_val_tokens_fr,maxlen=seq_len, padding='post')\n",
        "X_test_pad_fr = pad_sequences(X_test_tokens_fr,maxlen=seq_len, padding='post')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X22q8A2r8rmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "04f1de98-708f-49fe-aa05-b664ee6abd67"
      },
      "source": [
        "import pickle\n",
        "french = True\n",
        "german = True\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "if french:\n",
        "    fr_git_embed = pickle.load( open(path + '/embeddings/fr_slim_embed_ext.p', \"rb\" ) ) #fr_slim_embed_ext #fr_muse_align #fr_muse\n",
        "    #fr_model = KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/Thesis_ecb_ecoicop/embeddings/wiki.fr.vec')\n",
        "if german:\n",
        "    de_git_embed = pickle.load( open(path + '/embeddings/de_slim_embed_ext.p', \"rb\" ) ) #de_muse\n",
        "    #de_model = KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/Thesis_ecb_ecoicop/embeddings/wiki.de.vec')\n",
        "\n",
        "v = np.zeros(300)\n",
        "v[0]=1\n",
        "fr_git_embed['<sep>'] = v\n",
        "de_git_embed['<sep>'] = v\n",
        "v[0]=0\n",
        "v[1]=1\n",
        "fr_git_embed['<fr>'] = v\n",
        "v[1]=-1\n",
        "de_git_embed['<de>'] = v\n",
        "\n",
        "print('de_git_embed',len(de_git_embed.keys()))\n",
        "print('fr_git_embed',len(fr_git_embed.keys()))\n",
        "\n",
        "X_train_emb_de = np.array(list(model_helper.text_to_embed(X_train_de, ['de' for a in X_train_de], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_val_emb_de = np.array(list(model_helper.text_to_embed(X_val_de, ['de' for a in X_val_de], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_test_emb_de = np.array(list(model_helper.text_to_embed(X_test_de, ['de' for a in X_test_de], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "\n",
        "X_train_emb_fr = np.array(list(model_helper.text_to_embed(X_train_fr, ['fr' for a in X_train_fr], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_val_emb_fr = np.array(list(model_helper.text_to_embed(X_val_fr, ['fr' for a in X_val_fr], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_test_emb_fr = np.array(list(model_helper.text_to_embed(X_test_fr, ['fr' for a in X_test_fr], de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "\n",
        "print('de',X_train_emb_de.shape, X_val_emb_de.shape, X_test_emb_de.shape)\n",
        "print('fr',X_train_emb_fr.shape, X_val_emb_fr.shape, X_test_emb_fr.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "664it [00:00, 6639.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "de_git_embed 64413\n",
            "fr_git_embed 43014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16548it [00:02, 6158.69it/s]\n",
            "4137it [00:00, 6478.17it/s]\n",
            "1379it [00:00, 5994.88it/s]\n",
            "5418it [00:00, 6822.64it/s]\n",
            "1355it [00:00, 6612.09it/s]\n",
            "452it [00:00, 6596.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "de (16548, 39, 300) (4137, 39, 300) (1379, 39, 300)\n",
            "fr (5418, 39, 300) (1355, 39, 300) (452, 39, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjZke3ew9XYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "920ffa58-3490-494c-8205-14fb5e2e30b6"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_de['cc5'])\n",
        "\n",
        "def encode_label(y_):\n",
        "    y__ = encoder.transform(y_)\n",
        "    y_enc =tf.keras.utils.to_categorical(y__, num_classes=no_Classes, dtype=\"float32\")\n",
        "    return y_enc\n",
        "\n",
        "y_train_enc_de = encode_label(y_train_de)\n",
        "y_val_enc_de = encode_label(y_val_de)\n",
        "y_test_enc_de = encode_label(y_test_de)\n",
        "\n",
        "y_train_enc_fr = encode_label(y_train_fr)\n",
        "y_val_enc_fr = encode_label(y_val_fr)\n",
        "y_test_enc_fr = encode_label(y_test_fr)\n",
        "\n",
        "print(y_train_enc_de.shape,y_val_enc_de.shape,y_test_enc_de.shape)\n",
        "print(y_train_enc_fr.shape,y_val_enc_fr.shape,y_test_enc_fr.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16548, 74) (4137, 74) (1379, 74)\n",
            "(5418, 74) (1355, 74) (452, 74)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOG6Bc_RVe10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weigth_dict(y_train):\n",
        "    weights_dict = dict(zip(y_train.value_counts().index.tolist(),list(len(y_train) / ( len(y_train.unique())  * y_train.value_counts()))))\n",
        "\n",
        "    class_weight_dict = {}\n",
        "    for n,lab in enumerate(encoder.classes_):\n",
        "        try:\n",
        "            class_weight_dict[n] = weights_dict[lab]\n",
        "        except:\n",
        "            class_weight_dict[n] = 1\n",
        "    return class_weight_dict\n",
        "\n",
        "class_weight_dict_de = get_weigth_dict(y_train_de)\n",
        "class_weight_dict_fr = get_weigth_dict(y_train_fr)\n",
        "class_weight_dict_de_fr = get_weigth_dict(y_train_de.append(y_train_fr))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so7nxPzl5dKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unknown = np.random.rand(embedding_dim)\n",
        "\n",
        "def get_embed_matrix(embed,tokenizer,vocab_size,embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word,i in tokenizer.word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embed[word]\n",
        "        except KeyError:\n",
        "            #next\n",
        "            embedding_matrix[i] = unknown #np.random.rand(embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix_de = get_embed_matrix(de_git_embed,tokenizer_de,vocab_size_de,embedding_dim)\n",
        "embedding_matrix_fr = get_embed_matrix(fr_git_embed,tokenizer_fr,vocab_size_fr,embedding_dim)\n",
        "\n",
        "combine_embed = de_git_embed\n",
        "combine_embed.update(fr_git_embed)\n",
        "embedding_matrix_de_fr = get_embed_matrix(combine_embed,tokenizer_de_fr,vocab_size_de_fr,embedding_dim)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09DZu4q4nouv",
        "colab_type": "text"
      },
      "source": [
        "# Single Language Classifier German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upNTfEpdntqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, balanced_accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2q91oJWnttV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_idf_log_reg(X_train,y_train,X_test,y_test,no,weights_dict):\n",
        "    X_train = [str(x).replace('<','').replace('>','') for x in X_train[:no]]\n",
        "    y_train = y_train[:no]\n",
        "    X_test = [str(x).replace('<','').replace('>','') for x in X_test]\n",
        "    vectorizer  = TfidfVectorizer()\n",
        "    vectorizer.fit(X_train)\n",
        "    X_train_vec = vectorizer.transform(X_train)\n",
        "    X_test_vec  = vectorizer.transform(X_test)\n",
        "\n",
        "    logreg = LogisticRegression(C=1,max_iter=500, solver='newton-cg')#,class_weight=weights_dict)\n",
        "    logreg.fit(X_train_vec, y_train)\n",
        "\n",
        "    y_pred_train = logreg.predict(X_train_vec)\n",
        "    y_pred_test = logreg.predict(X_test_vec)\n",
        "\n",
        "    print('train obs.',no,'accuracy %s' % accuracy_score(y_pred_train, y_train))\n",
        "    print('train obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_train, y_train))\n",
        "    print('test obs.',no,'accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))\n",
        "    print()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNe84VIOxHo",
        "colab_type": "text"
      },
      "source": [
        "##LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsi7UOpeiGDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "72c21e12-537d-41c5-ef57-9d29e127b4df"
      },
      "source": [
        "tf_idf_log_reg(X_train_de,y_train_de,X_test_de,y_test_de,7500,class_weight_dict_de)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train obs. 7500 accuracy 0.948\n",
            "train obs. 7500 b_accuracy 0.9554857711222235\n",
            "test obs. 7500 accuracy 0.9093546047860769\n",
            "test obs. 7500 b_accuracy 0.9008351224575238\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IVDbZDzn4zv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "47fcb4fe-9912-439e-f8dd-f6efb80d08ab"
      },
      "source": [
        "stra = False\n",
        "for obs in [250,500,1000,2000,5000,10000,15000]:\n",
        "    tf_idf_log_reg(X_train_de,y_train_de,X_test_de,y_test_de,obs,class_weight_dict_de)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train obs. 250 accuracy 0.728\n",
            "train obs. 250 b_accuracy 0.9227584603585467\n",
            "test obs. 250 accuracy 0.3727338651196519\n",
            "test obs. 250 b_accuracy 0.7807774081653794\n",
            "\n",
            "train obs. 500 accuracy 0.754\n",
            "train obs. 500 b_accuracy 0.9356016875681188\n",
            "test obs. 500 accuracy 0.48223350253807107\n",
            "test obs. 500 b_accuracy 0.819003535956219\n",
            "\n",
            "train obs. 1000 accuracy 0.861\n",
            "train obs. 1000 b_accuracy 0.944278209737142\n",
            "test obs. 1000 accuracy 0.6932559825960841\n",
            "test obs. 1000 b_accuracy 0.8707367800131294\n",
            "\n",
            "train obs. 2000 accuracy 0.9015\n",
            "train obs. 2000 b_accuracy 0.9393502431781049\n",
            "test obs. 2000 accuracy 0.8158085569253082\n",
            "test obs. 2000 b_accuracy 0.8771264885273545\n",
            "\n",
            "train obs. 5000 accuracy 0.9392\n",
            "train obs. 5000 b_accuracy 0.9484502782358264\n",
            "test obs. 5000 accuracy 0.8905003625815808\n",
            "test obs. 5000 b_accuracy 0.9027686807727505\n",
            "\n",
            "train obs. 10000 accuracy 0.9535\n",
            "train obs. 10000 b_accuracy 0.957200699258984\n",
            "test obs. 10000 accuracy 0.9187817258883249\n",
            "test obs. 10000 b_accuracy 0.9035734622541813\n",
            "\n",
            "train obs. 15000 accuracy 0.9618666666666666\n",
            "train obs. 15000 b_accuracy 0.9596541097518718\n",
            "test obs. 15000 accuracy 0.9376359680928209\n",
            "test obs. 15000 b_accuracy 0.9209134587904934\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7FiZZp6OvSB",
        "colab_type": "text"
      },
      "source": [
        "## NN Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1eMhccQBtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense,Embedding, Flatten,Reshape, GlobalAveragePooling1D,MaxPool2D, Concatenate,Conv1D,Conv2D,BatchNormalization,Add,Masking,GlobalMaxPooling1D,LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.regularizers import l1,l2\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7KbLKMg634_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_CNN(X_train_pad,y_train_enc,X_val_pad,y_val_enc,X_test_pad,y_test,vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "    \n",
        "    dropout_rate=.5\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/75)\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), dtype = 'float')\n",
        "    embedd_seq = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = seq_len, trainable = True, mask_zero=False)(input_layer)\n",
        "    conv_layer = Conv1D(filters=100,   kernel_size= 5,   padding='valid',  activation='relu', strides=1)(embedd_seq)\n",
        "    pool_layer = GlobalMaxPooling1D()(conv_layer)\n",
        "    #pool_layer = GlobalAveragePooling1D()(embedd_seq)\n",
        "    drop_layer = Dropout(dropout_rate)(pool_layer)\n",
        "    pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "    avg_pool_mod = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "    avg_pool_mod.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = avg_pool_mod.fit(x = X_train_pad[:no], y = y_train_enc[:no],\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test = avg_pool_mod.predict(X_test_pad)\n",
        "    y_pred_arg = y_pred_test.argmax(axis=1)\n",
        "    y_pred_test= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "    print('test obs.',no,'accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty02iah0mBfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_CNN2D(X_train_pad,y_train_enc,X_val_pad,y_val_enc,X_test_pad,y_test,vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "    \n",
        "    dropout_rate= .5\n",
        "    filter_sizes = [1,3,5,7,9,11,13]\n",
        "    num_filters = 11\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), name='text_input')\n",
        "    x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable = True)(input_layer)\n",
        "    x = Reshape((seq_len, embedding_dim, 1))(x)\n",
        "\n",
        "    maxpool_pool = []\n",
        "    for i in range(len(filter_sizes)):\n",
        "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_dim),\n",
        "                                      kernel_initializer='he_normal', activation='relu')(x)\n",
        "        maxpool_pool.append(MaxPool2D(pool_size=(seq_len - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "    pool_layer = Concatenate(axis=1)(maxpool_pool)   \n",
        "\n",
        "    #conv_layer = Conv2D(filters=100,   kernel_size=(5,300),   padding='same',  activation='relu', strides=1,name='convolution')(x)\n",
        "    #pool_layer = MaxPool2D(pool_size = (35,1),name='max_pooling')(conv_layer)\n",
        "    falt_layer = Flatten(name='flat')(pool_layer)\n",
        "    drop_layer = Dropout(dropout_rate,name='drop')(falt_layer)\n",
        "    pred_layer = Dense(no_Classes, activation = 'softmax', name='output_de')(drop_layer) \n",
        "\n",
        "    de_cnn2d = Model(inputs = [input_layer], outputs = [pred_layer])\n",
        "    #de_cnn2d.summary()\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "    de_cnn2d.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = de_cnn2d.fit(x = X_train_pad[:no], y = y_train_enc[:no],\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 100, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test = de_cnn2d.predict(X_test_pad)\n",
        "    y_pred_arg = y_pred_test.argmax(axis=1)\n",
        "    y_pred_test= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "    print('test obs.',no,'accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHsjtwzX64cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_RNN(X_train_pad,y_train_enc,X_val_pad,y_val_enc,X_test_pad,y_test,vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "    dropout_rate=.5\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), dtype = 'int32')\n",
        "    embedd_seq = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = seq_len, trainable = True)(input_layer)\n",
        "    lstm_layer = Bidirectional(LSTM(128,activation = 'tanh',recurrent_activation = 'sigmoid', dropout = dropout_rate,recurrent_dropout = 0, unroll = False,use_bias = True))(embedd_seq)\n",
        "    drop_layer = Dropout(dropout_rate)(lstm_layer)\n",
        "    pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "    avg_pool_mod = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "    avg_pool_mod.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = avg_pool_mod.fit(x = X_train_pad[:no], y = y_train_enc[:no],\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test = avg_pool_mod.predict(X_test_pad)\n",
        "    y_pred_arg = y_pred_test.argmax(axis=1)\n",
        "    y_pred_test= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "    print('test obs.',no,'accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))\n",
        "    print()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1E-j4fIQJTa",
        "colab_type": "text"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3tEI_3HQNSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f05bb1a-3c66-4d20-eef0-44f114347e65"
      },
      "source": [
        "for obs in [500,1000,2000,5000,10000,15000]:\n",
        "     run_CNN(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 4.2286 - accuracy: 0.0140 - val_loss: 4.2940 - val_accuracy: 0.0544\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 158ms/step - loss: 3.9401 - accuracy: 0.0360 - val_loss: 4.2567 - val_accuracy: 0.0051\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 3.6723 - accuracy: 0.0340 - val_loss: 4.1654 - val_accuracy: 0.0213\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 3.4897 - accuracy: 0.0520 - val_loss: 4.0852 - val_accuracy: 0.0771\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 3.2847 - accuracy: 0.1160 - val_loss: 4.0239 - val_accuracy: 0.0935\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 3.0997 - accuracy: 0.1680 - val_loss: 3.9226 - val_accuracy: 0.0844\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 2.9898 - accuracy: 0.2020 - val_loss: 3.8275 - val_accuracy: 0.1160\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.7320 - accuracy: 0.2420 - val_loss: 3.7061 - val_accuracy: 0.1827\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 2.5250 - accuracy: 0.3160 - val_loss: 3.5686 - val_accuracy: 0.2403\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.3294 - accuracy: 0.3720 - val_loss: 3.4202 - val_accuracy: 0.2751\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 2.0189 - accuracy: 0.4520 - val_loss: 3.2485 - val_accuracy: 0.3128\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 1.8712 - accuracy: 0.5040 - val_loss: 3.0726 - val_accuracy: 0.3476\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 1.6132 - accuracy: 0.6040 - val_loss: 2.9165 - val_accuracy: 0.3735\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 130ms/step - loss: 1.3732 - accuracy: 0.6720 - val_loss: 2.7532 - val_accuracy: 0.4114\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 1.1759 - accuracy: 0.7220 - val_loss: 2.6013 - val_accuracy: 0.4617\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 1.0108 - accuracy: 0.7440 - val_loss: 2.4628 - val_accuracy: 0.4931\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 131ms/step - loss: 0.8133 - accuracy: 0.8040 - val_loss: 2.3506 - val_accuracy: 0.5153\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 0.8122 - accuracy: 0.7900 - val_loss: 2.2464 - val_accuracy: 0.5364\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 0.6273 - accuracy: 0.8340 - val_loss: 2.1610 - val_accuracy: 0.5586\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.5267 - accuracy: 0.8760 - val_loss: 2.0862 - val_accuracy: 0.5748\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 0.4843 - accuracy: 0.8940 - val_loss: 2.0272 - val_accuracy: 0.5840\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 0.3843 - accuracy: 0.9080 - val_loss: 1.9857 - val_accuracy: 0.5922\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.3586 - accuracy: 0.9020 - val_loss: 1.9510 - val_accuracy: 0.5980\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 0.2967 - accuracy: 0.9180 - val_loss: 1.9101 - val_accuracy: 0.6058\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.2455 - accuracy: 0.9260 - val_loss: 1.8624 - val_accuracy: 0.6198\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.2135 - accuracy: 0.9440 - val_loss: 1.8172 - val_accuracy: 0.6345\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 132ms/step - loss: 0.2008 - accuracy: 0.9440 - val_loss: 1.7887 - val_accuracy: 0.6381\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.1703 - accuracy: 0.9440 - val_loss: 1.7676 - val_accuracy: 0.6442\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.1523 - accuracy: 0.9600 - val_loss: 1.7539 - val_accuracy: 0.6500\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.1415 - accuracy: 0.9580 - val_loss: 1.7394 - val_accuracy: 0.6543\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 0.1317 - accuracy: 0.9560 - val_loss: 1.7246 - val_accuracy: 0.6582\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 0.1307 - accuracy: 0.9720 - val_loss: 1.7130 - val_accuracy: 0.6630\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 0.1056 - accuracy: 0.9780 - val_loss: 1.7092 - val_accuracy: 0.6662\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 125ms/step - loss: 0.1121 - accuracy: 0.9760 - val_loss: 1.7159 - val_accuracy: 0.6575\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 0.0914 - accuracy: 0.9740 - val_loss: 1.7139 - val_accuracy: 0.6570\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 126ms/step - loss: 0.0633 - accuracy: 0.9800 - val_loss: 1.7123 - val_accuracy: 0.6565\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.0912 - accuracy: 0.9700 - val_loss: 1.7148 - val_accuracy: 0.6580\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 129ms/step - loss: 0.0763 - accuracy: 0.9780 - val_loss: 1.7222 - val_accuracy: 0.6606\n",
            "test obs. 500 accuracy 0.6700507614213198\n",
            "test obs. 500 b_accuracy 0.5886799708120483\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 135ms/step - loss: 4.3037 - accuracy: 0.0320 - val_loss: 4.1792 - val_accuracy: 0.0904\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 3.8916 - accuracy: 0.0600 - val_loss: 4.1133 - val_accuracy: 0.0703\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 101ms/step - loss: 3.6321 - accuracy: 0.0830 - val_loss: 3.9616 - val_accuracy: 0.0788\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 3.2812 - accuracy: 0.1630 - val_loss: 3.7329 - val_accuracy: 0.1760\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 3.0067 - accuracy: 0.2480 - val_loss: 3.4832 - val_accuracy: 0.2475\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 2.6280 - accuracy: 0.3180 - val_loss: 3.2147 - val_accuracy: 0.3164\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 2.3044 - accuracy: 0.4250 - val_loss: 2.9063 - val_accuracy: 0.3698\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 1.9218 - accuracy: 0.5380 - val_loss: 2.5806 - val_accuracy: 0.4564\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 1.5620 - accuracy: 0.6220 - val_loss: 2.2878 - val_accuracy: 0.5260\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 1.2924 - accuracy: 0.7020 - val_loss: 2.0228 - val_accuracy: 0.5883\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.9878 - accuracy: 0.7340 - val_loss: 1.8169 - val_accuracy: 0.6282\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.7934 - accuracy: 0.8080 - val_loss: 1.6427 - val_accuracy: 0.6734\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.5738 - accuracy: 0.8680 - val_loss: 1.5181 - val_accuracy: 0.6918\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.4998 - accuracy: 0.8770 - val_loss: 1.4087 - val_accuracy: 0.7053\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.4145 - accuracy: 0.8930 - val_loss: 1.3262 - val_accuracy: 0.7254\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.3342 - accuracy: 0.9180 - val_loss: 1.2557 - val_accuracy: 0.7416\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.2416 - accuracy: 0.9340 - val_loss: 1.2122 - val_accuracy: 0.7491\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 0.2457 - accuracy: 0.9310 - val_loss: 1.1883 - val_accuracy: 0.7489\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.2346 - accuracy: 0.9310 - val_loss: 1.1724 - val_accuracy: 0.7542\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.2033 - accuracy: 0.9410 - val_loss: 1.1497 - val_accuracy: 0.7580\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 99ms/step - loss: 0.1615 - accuracy: 0.9580 - val_loss: 1.1237 - val_accuracy: 0.7636\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 0.1606 - accuracy: 0.9540 - val_loss: 1.1145 - val_accuracy: 0.7687\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.1384 - accuracy: 0.9620 - val_loss: 1.1019 - val_accuracy: 0.7723\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.1149 - accuracy: 0.9670 - val_loss: 1.0868 - val_accuracy: 0.7757\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.1062 - accuracy: 0.9680 - val_loss: 1.0703 - val_accuracy: 0.7803\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.0963 - accuracy: 0.9690 - val_loss: 1.0804 - val_accuracy: 0.7733\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 0.1007 - accuracy: 0.9720 - val_loss: 1.0725 - val_accuracy: 0.7747\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 0.1154 - accuracy: 0.9650 - val_loss: 1.0565 - val_accuracy: 0.7844\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.0997 - accuracy: 0.9710 - val_loss: 1.0439 - val_accuracy: 0.7885\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 98ms/step - loss: 0.0757 - accuracy: 0.9780 - val_loss: 1.0430 - val_accuracy: 0.7928\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0865 - accuracy: 0.9730 - val_loss: 1.0491 - val_accuracy: 0.7904\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 97ms/step - loss: 0.0572 - accuracy: 0.9850 - val_loss: 1.0501 - val_accuracy: 0.7909\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 95ms/step - loss: 0.0775 - accuracy: 0.9810 - val_loss: 1.0386 - val_accuracy: 0.7936\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0699 - accuracy: 0.9830 - val_loss: 1.0354 - val_accuracy: 0.7943\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 0.0450 - accuracy: 0.9850 - val_loss: 1.0297 - val_accuracy: 0.7960\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0517 - accuracy: 0.9880 - val_loss: 1.0212 - val_accuracy: 0.7984\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 93ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 1.0189 - val_accuracy: 0.8013\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 93ms/step - loss: 0.0458 - accuracy: 0.9870 - val_loss: 1.0265 - val_accuracy: 0.7957\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0588 - accuracy: 0.9790 - val_loss: 1.0371 - val_accuracy: 0.7945\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 1.0398 - val_accuracy: 0.7941\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 0.0610 - accuracy: 0.9780 - val_loss: 1.0500 - val_accuracy: 0.7926\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 94ms/step - loss: 0.0510 - accuracy: 0.9850 - val_loss: 1.0506 - val_accuracy: 0.7972\n",
            "test obs. 1000 accuracy 0.8150833937635968\n",
            "test obs. 1000 b_accuracy 0.7470289975856765\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 1s 97ms/step - loss: 4.6700 - accuracy: 0.0190 - val_loss: 4.2161 - val_accuracy: 0.0416\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 1s 79ms/step - loss: 4.1117 - accuracy: 0.0735 - val_loss: 3.9249 - val_accuracy: 0.1332\n",
            "Epoch 3/50\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 3.6909 - accuracy: 0.1451"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5a48239e129e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m      \u001b[0mrun_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pad_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_enc_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val_pad_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_enc_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_pad_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight_dict_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-6d61a32f1a26>\u001b[0m in \u001b[0;36mrun_CNN\u001b[0;34m(X_train_pad, y_train_enc, X_val_pad, y_val_enc, X_test_pad, y_test, vocab_size, embedding_matrix, class_weight_dict, no)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_pool_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight_dict\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-1K5rtvmvcg",
        "colab_type": "text"
      },
      "source": [
        "## CNN2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "893RDiv5mqOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbc3ac73-aabb-47d4-ff58-457994469d3d"
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000,10000,15000]:\n",
        "     run_CNN2D(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 589ms/step - loss: 3.5526 - accuracy: 0.0000e+00 - val_loss: 4.4865 - val_accuracy: 0.0295\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 3.5687 - accuracy: 0.0000e+00 - val_loss: 4.4419 - val_accuracy: 0.0237\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 3.0838 - accuracy: 0.0600 - val_loss: 4.4358 - val_accuracy: 0.0329\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 407ms/step - loss: 2.8055 - accuracy: 0.0900 - val_loss: 4.4048 - val_accuracy: 0.0220\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 2.9749 - accuracy: 0.0800 - val_loss: 4.3893 - val_accuracy: 0.0143\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 721ms/step - loss: 2.7339 - accuracy: 0.0900 - val_loss: 4.3398 - val_accuracy: 0.0230\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 2.3966 - accuracy: 0.1700 - val_loss: 4.2910 - val_accuracy: 0.0365\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 2.3521 - accuracy: 0.1900 - val_loss: 4.2574 - val_accuracy: 0.0732\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 2.0660 - accuracy: 0.2800 - val_loss: 4.2353 - val_accuracy: 0.0979\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 1.9842 - accuracy: 0.3200 - val_loss: 4.2273 - val_accuracy: 0.1006\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 1.9150 - accuracy: 0.3300 - val_loss: 4.2287 - val_accuracy: 0.1035\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 1.7274 - accuracy: 0.3900 - val_loss: 4.2256 - val_accuracy: 0.1080\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 405ms/step - loss: 1.5620 - accuracy: 0.5100 - val_loss: 4.2303 - val_accuracy: 0.1126\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 404ms/step - loss: 1.4694 - accuracy: 0.5000 - val_loss: 4.2153 - val_accuracy: 0.1262\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 1.3932 - accuracy: 0.4800 - val_loss: 4.1973 - val_accuracy: 0.1424\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 1.1820 - accuracy: 0.6000 - val_loss: 4.1855 - val_accuracy: 0.1518\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 1.0191 - accuracy: 0.6700 - val_loss: 4.1923 - val_accuracy: 0.1687\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 0.9870 - accuracy: 0.6700 - val_loss: 4.2089 - val_accuracy: 0.1745\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.8822 - accuracy: 0.7200 - val_loss: 4.2503 - val_accuracy: 0.1769\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.8104 - accuracy: 0.7400 - val_loss: 4.2856 - val_accuracy: 0.1767\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.7678 - accuracy: 0.7600 - val_loss: 4.3051 - val_accuracy: 0.1716\n",
            "test obs. 100 accuracy 0.1646120377084844\n",
            "test obs. 100 b_accuracy 0.3377735993104644\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 4.1960 - accuracy: 0.0120 - val_loss: 4.6002 - val_accuracy: 0.0031\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 4.1017 - accuracy: 0.0160 - val_loss: 4.3622 - val_accuracy: 0.0036\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 3.5758 - accuracy: 0.0280 - val_loss: 4.3138 - val_accuracy: 0.0121\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 3.5997 - accuracy: 0.0280 - val_loss: 4.2527 - val_accuracy: 0.0145\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 3.3689 - accuracy: 0.0760 - val_loss: 4.2039 - val_accuracy: 0.0097\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 3.3025 - accuracy: 0.0640 - val_loss: 4.1680 - val_accuracy: 0.0271\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 3.2526 - accuracy: 0.0840 - val_loss: 4.1420 - val_accuracy: 0.0609\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 3.0590 - accuracy: 0.1040 - val_loss: 4.1002 - val_accuracy: 0.0740\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 2.8322 - accuracy: 0.1880 - val_loss: 4.0748 - val_accuracy: 0.0745\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 2.7855 - accuracy: 0.1600 - val_loss: 4.0755 - val_accuracy: 0.0633\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 402ms/step - loss: 2.6602 - accuracy: 0.1680 - val_loss: 4.0830 - val_accuracy: 0.0665\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 2.3913 - accuracy: 0.2400 - val_loss: 4.0985 - val_accuracy: 0.0595\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 2.4493 - accuracy: 0.1920 - val_loss: 4.0570 - val_accuracy: 0.0631\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 2.2795 - accuracy: 0.2320 - val_loss: 3.9738 - val_accuracy: 0.0984\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 2.1806 - accuracy: 0.2960 - val_loss: 3.8961 - val_accuracy: 0.1286\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 1.9832 - accuracy: 0.3240 - val_loss: 3.8511 - val_accuracy: 0.1402\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 1.9280 - accuracy: 0.4480 - val_loss: 3.8094 - val_accuracy: 0.1458\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 1.8382 - accuracy: 0.4720 - val_loss: 3.7683 - val_accuracy: 0.1588\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 1.7225 - accuracy: 0.4680 - val_loss: 3.7276 - val_accuracy: 0.1709\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 1.4887 - accuracy: 0.5720 - val_loss: 3.6885 - val_accuracy: 0.1798\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 1.4146 - accuracy: 0.5800 - val_loss: 3.6448 - val_accuracy: 0.1975\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 1.3739 - accuracy: 0.6080 - val_loss: 3.5933 - val_accuracy: 0.2219\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 409ms/step - loss: 1.2582 - accuracy: 0.6120 - val_loss: 3.5299 - val_accuracy: 0.2458\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 1.2051 - accuracy: 0.6680 - val_loss: 3.4591 - val_accuracy: 0.2806\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.9500 - accuracy: 0.7640 - val_loss: 3.3973 - val_accuracy: 0.3077\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.9570 - accuracy: 0.7480 - val_loss: 3.3443 - val_accuracy: 0.3258\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.8106 - accuracy: 0.8120 - val_loss: 3.3014 - val_accuracy: 0.3372\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.7803 - accuracy: 0.8360 - val_loss: 3.2659 - val_accuracy: 0.3483\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.6770 - accuracy: 0.8400 - val_loss: 3.2387 - val_accuracy: 0.3541\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.6001 - accuracy: 0.8480 - val_loss: 3.2135 - val_accuracy: 0.3621\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.5969 - accuracy: 0.8480 - val_loss: 3.1921 - val_accuracy: 0.3701\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.5595 - accuracy: 0.8320 - val_loss: 3.1638 - val_accuracy: 0.3783\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 410ms/step - loss: 0.5074 - accuracy: 0.8560 - val_loss: 3.1306 - val_accuracy: 0.3916\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 416ms/step - loss: 0.4578 - accuracy: 0.9000 - val_loss: 3.1025 - val_accuracy: 0.3981\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 422ms/step - loss: 0.4493 - accuracy: 0.8760 - val_loss: 3.0747 - val_accuracy: 0.4046\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.3703 - accuracy: 0.9120 - val_loss: 3.0575 - val_accuracy: 0.4109\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.2909 - accuracy: 0.9400 - val_loss: 3.0508 - val_accuracy: 0.4158\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.3208 - accuracy: 0.9160 - val_loss: 3.0462 - val_accuracy: 0.4172\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.3086 - accuracy: 0.9360 - val_loss: 3.0397 - val_accuracy: 0.4175\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.2775 - accuracy: 0.9400 - val_loss: 3.0397 - val_accuracy: 0.4162\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.2204 - accuracy: 0.9400 - val_loss: 3.0413 - val_accuracy: 0.4182\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.2252 - accuracy: 0.9400 - val_loss: 3.0378 - val_accuracy: 0.4237\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.2218 - accuracy: 0.9400 - val_loss: 3.0329 - val_accuracy: 0.4262\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 413ms/step - loss: 0.1721 - accuracy: 0.9640 - val_loss: 3.0247 - val_accuracy: 0.4317\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.2246 - accuracy: 0.9520 - val_loss: 3.0132 - val_accuracy: 0.4339\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.1651 - accuracy: 0.9640 - val_loss: 3.0047 - val_accuracy: 0.4394\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.1519 - accuracy: 0.9600 - val_loss: 2.9964 - val_accuracy: 0.4433\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 0.1176 - accuracy: 0.9720 - val_loss: 2.9914 - val_accuracy: 0.4484\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.1859 - accuracy: 0.9440 - val_loss: 2.9858 - val_accuracy: 0.4518\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.1188 - accuracy: 0.9760 - val_loss: 2.9839 - val_accuracy: 0.4559\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 0.1475 - accuracy: 0.9560 - val_loss: 2.9807 - val_accuracy: 0.4573\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.1383 - accuracy: 0.9560 - val_loss: 2.9771 - val_accuracy: 0.4588\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 428ms/step - loss: 0.1742 - accuracy: 0.9360 - val_loss: 2.9792 - val_accuracy: 0.4590\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.1244 - accuracy: 0.9640 - val_loss: 2.9770 - val_accuracy: 0.4588\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 418ms/step - loss: 0.1161 - accuracy: 0.9720 - val_loss: 2.9768 - val_accuracy: 0.4554\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 411ms/step - loss: 0.0878 - accuracy: 0.9680 - val_loss: 2.9766 - val_accuracy: 0.4571\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 0.0989 - accuracy: 0.9680 - val_loss: 2.9771 - val_accuracy: 0.4593\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 414ms/step - loss: 0.0964 - accuracy: 0.9600 - val_loss: 2.9777 - val_accuracy: 0.4598\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.1031 - accuracy: 0.9760 - val_loss: 2.9764 - val_accuracy: 0.4602\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 417ms/step - loss: 0.1074 - accuracy: 0.9800 - val_loss: 2.9776 - val_accuracy: 0.4590\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 408ms/step - loss: 0.1232 - accuracy: 0.9600 - val_loss: 2.9839 - val_accuracy: 0.4576\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 423ms/step - loss: 0.0817 - accuracy: 0.9680 - val_loss: 2.9897 - val_accuracy: 0.4583\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 412ms/step - loss: 0.0734 - accuracy: 0.9840 - val_loss: 2.9955 - val_accuracy: 0.4578\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 401ms/step - loss: 0.0783 - accuracy: 0.9800 - val_loss: 3.0010 - val_accuracy: 0.4588\n",
            "test obs. 250 accuracy 0.4757070340826686\n",
            "test obs. 250 b_accuracy 0.5135627131738715\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 1s 338ms/step - loss: 4.3037 - accuracy: 0.0160 - val_loss: 4.3639 - val_accuracy: 0.0128\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 254ms/step - loss: 3.9465 - accuracy: 0.0120 - val_loss: 4.2556 - val_accuracy: 0.0179\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 3.7475 - accuracy: 0.0340 - val_loss: 4.1764 - val_accuracy: 0.0682\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 3.5082 - accuracy: 0.0780 - val_loss: 4.1226 - val_accuracy: 0.0645\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 251ms/step - loss: 3.4174 - accuracy: 0.1040 - val_loss: 4.0464 - val_accuracy: 0.0720\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 247ms/step - loss: 3.2446 - accuracy: 0.1540 - val_loss: 3.9646 - val_accuracy: 0.0793\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 247ms/step - loss: 3.0624 - accuracy: 0.1740 - val_loss: 3.8943 - val_accuracy: 0.0945\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 248ms/step - loss: 2.9085 - accuracy: 0.2000 - val_loss: 3.7823 - val_accuracy: 0.1240\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 2.6981 - accuracy: 0.2720 - val_loss: 3.6560 - val_accuracy: 0.1796\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 248ms/step - loss: 2.5311 - accuracy: 0.3580 - val_loss: 3.5204 - val_accuracy: 0.2217\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 2.3515 - accuracy: 0.4120 - val_loss: 3.4297 - val_accuracy: 0.2323\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 2.1553 - accuracy: 0.4300 - val_loss: 3.3332 - val_accuracy: 0.2705\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 1.8868 - accuracy: 0.5000 - val_loss: 3.2179 - val_accuracy: 0.2922\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 1.6933 - accuracy: 0.5600 - val_loss: 3.0623 - val_accuracy: 0.3408\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 254ms/step - loss: 1.4854 - accuracy: 0.6280 - val_loss: 2.9310 - val_accuracy: 0.3689\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 250ms/step - loss: 1.3645 - accuracy: 0.6680 - val_loss: 2.8140 - val_accuracy: 0.3882\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 248ms/step - loss: 1.1742 - accuracy: 0.6800 - val_loss: 2.6882 - val_accuracy: 0.4237\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 1.0336 - accuracy: 0.7220 - val_loss: 2.5658 - val_accuracy: 0.4477\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 250ms/step - loss: 0.9290 - accuracy: 0.7220 - val_loss: 2.4559 - val_accuracy: 0.4704\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 0.7594 - accuracy: 0.8100 - val_loss: 2.3578 - val_accuracy: 0.4958\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 251ms/step - loss: 0.7358 - accuracy: 0.8100 - val_loss: 2.2781 - val_accuracy: 0.5168\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.6056 - accuracy: 0.8300 - val_loss: 2.2263 - val_accuracy: 0.5267\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 0.5355 - accuracy: 0.8700 - val_loss: 2.1733 - val_accuracy: 0.5342\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 253ms/step - loss: 0.4835 - accuracy: 0.8440 - val_loss: 2.1193 - val_accuracy: 0.5424\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 0.4454 - accuracy: 0.8700 - val_loss: 2.0662 - val_accuracy: 0.5567\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.4465 - accuracy: 0.8580 - val_loss: 2.0160 - val_accuracy: 0.5741\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.3755 - accuracy: 0.8880 - val_loss: 1.9759 - val_accuracy: 0.5842\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 251ms/step - loss: 0.2874 - accuracy: 0.9140 - val_loss: 1.9544 - val_accuracy: 0.5857\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 251ms/step - loss: 0.3118 - accuracy: 0.9100 - val_loss: 1.9571 - val_accuracy: 0.5859\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.3064 - accuracy: 0.9080 - val_loss: 1.9551 - val_accuracy: 0.5876\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 255ms/step - loss: 0.2357 - accuracy: 0.9220 - val_loss: 1.9468 - val_accuracy: 0.5915\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 0.2476 - accuracy: 0.9180 - val_loss: 1.9206 - val_accuracy: 0.5971\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 245ms/step - loss: 0.2255 - accuracy: 0.9280 - val_loss: 1.8884 - val_accuracy: 0.6021\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 250ms/step - loss: 0.2255 - accuracy: 0.9240 - val_loss: 1.8695 - val_accuracy: 0.6091\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 255ms/step - loss: 0.2318 - accuracy: 0.9200 - val_loss: 1.8641 - val_accuracy: 0.6089\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 0.1691 - accuracy: 0.9500 - val_loss: 1.8704 - val_accuracy: 0.6084\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 246ms/step - loss: 0.1709 - accuracy: 0.9620 - val_loss: 1.8855 - val_accuracy: 0.6106\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 249ms/step - loss: 0.1505 - accuracy: 0.9560 - val_loss: 1.8971 - val_accuracy: 0.6135\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 254ms/step - loss: 0.1615 - accuracy: 0.9440 - val_loss: 1.8914 - val_accuracy: 0.6161\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 252ms/step - loss: 0.1162 - accuracy: 0.9640 - val_loss: 1.8711 - val_accuracy: 0.6241\n",
            "test obs. 500 accuracy 0.6345177664974619\n",
            "test obs. 500 b_accuracy 0.5868408205675266\n",
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 224ms/step - loss: 4.4016 - accuracy: 0.0140 - val_loss: 4.2432 - val_accuracy: 0.0201\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 1s 172ms/step - loss: 3.9051 - accuracy: 0.0470 - val_loss: 4.1838 - val_accuracy: 0.0604\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 1s 170ms/step - loss: 3.7405 - accuracy: 0.0550 - val_loss: 4.0337 - val_accuracy: 0.1373\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 3.4464 - accuracy: 0.1240 - val_loss: 3.8230 - val_accuracy: 0.1467\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 1s 172ms/step - loss: 3.1639 - accuracy: 0.1790 - val_loss: 3.6155 - val_accuracy: 0.1827\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 1s 170ms/step - loss: 2.9492 - accuracy: 0.2370 - val_loss: 3.4003 - val_accuracy: 0.2657\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 2.5817 - accuracy: 0.3070 - val_loss: 3.1582 - val_accuracy: 0.3466\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 2.2477 - accuracy: 0.4000 - val_loss: 2.8692 - val_accuracy: 0.4373\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 1s 174ms/step - loss: 1.9632 - accuracy: 0.5020 - val_loss: 2.5668 - val_accuracy: 0.4890\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 1.6389 - accuracy: 0.5710 - val_loss: 2.2690 - val_accuracy: 0.5482\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 1s 173ms/step - loss: 1.3398 - accuracy: 0.6640 - val_loss: 1.9984 - val_accuracy: 0.5908\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 1.1139 - accuracy: 0.6970 - val_loss: 1.7803 - val_accuracy: 0.6415\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 1s 172ms/step - loss: 0.9569 - accuracy: 0.7350 - val_loss: 1.6392 - val_accuracy: 0.6609\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 1s 175ms/step - loss: 0.7641 - accuracy: 0.7800 - val_loss: 1.5093 - val_accuracy: 0.6742\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 1s 173ms/step - loss: 0.6796 - accuracy: 0.7980 - val_loss: 1.4264 - val_accuracy: 0.6829\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.6210 - accuracy: 0.8230 - val_loss: 1.3746 - val_accuracy: 0.6879\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 1s 174ms/step - loss: 0.6383 - accuracy: 0.8160 - val_loss: 1.3170 - val_accuracy: 0.7005\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 1s 175ms/step - loss: 0.4671 - accuracy: 0.8460 - val_loss: 1.2688 - val_accuracy: 0.7203\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 1s 168ms/step - loss: 0.4139 - accuracy: 0.8760 - val_loss: 1.2338 - val_accuracy: 0.7244\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 1s 168ms/step - loss: 0.4024 - accuracy: 0.8830 - val_loss: 1.2030 - val_accuracy: 0.7314\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.3295 - accuracy: 0.9000 - val_loss: 1.1794 - val_accuracy: 0.7423\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 0.2984 - accuracy: 0.9070 - val_loss: 1.1557 - val_accuracy: 0.7520\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 0.2634 - accuracy: 0.9190 - val_loss: 1.1467 - val_accuracy: 0.7554\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 1s 170ms/step - loss: 0.2258 - accuracy: 0.9240 - val_loss: 1.1427 - val_accuracy: 0.7556\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.2492 - accuracy: 0.9240 - val_loss: 1.1326 - val_accuracy: 0.7605\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.2100 - accuracy: 0.9340 - val_loss: 1.1269 - val_accuracy: 0.7600\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 1s 168ms/step - loss: 0.1898 - accuracy: 0.9390 - val_loss: 1.1310 - val_accuracy: 0.7636\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.1902 - accuracy: 0.9440 - val_loss: 1.1172 - val_accuracy: 0.7641\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 1s 170ms/step - loss: 0.2285 - accuracy: 0.9370 - val_loss: 1.1081 - val_accuracy: 0.7713\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 0.2094 - accuracy: 0.9430 - val_loss: 1.1136 - val_accuracy: 0.7711\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 1s 171ms/step - loss: 0.1873 - accuracy: 0.9390 - val_loss: 1.1344 - val_accuracy: 0.7660\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 1s 169ms/step - loss: 0.1723 - accuracy: 0.9490 - val_loss: 1.1325 - val_accuracy: 0.7708\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 1s 172ms/step - loss: 0.1547 - accuracy: 0.9560 - val_loss: 1.1244 - val_accuracy: 0.7757\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 1s 172ms/step - loss: 0.1295 - accuracy: 0.9480 - val_loss: 1.1179 - val_accuracy: 0.7740\n",
            "test obs. 1000 accuracy 0.8013052936910805\n",
            "test obs. 1000 b_accuracy 0.7311997761856472\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 186ms/step - loss: 4.6551 - accuracy: 0.0180 - val_loss: 4.1676 - val_accuracy: 0.0778\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 4.1536 - accuracy: 0.0870 - val_loss: 3.8894 - val_accuracy: 0.1723\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 1s 128ms/step - loss: 3.7457 - accuracy: 0.1705 - val_loss: 3.4802 - val_accuracy: 0.2449\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 3.3183 - accuracy: 0.2600 - val_loss: 2.9807 - val_accuracy: 0.3604\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 2.7813 - accuracy: 0.3740 - val_loss: 2.4244 - val_accuracy: 0.5195\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 1s 169ms/step - loss: 2.1768 - accuracy: 0.5025 - val_loss: 1.8827 - val_accuracy: 0.6447\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 1.7664 - accuracy: 0.5790 - val_loss: 1.4568 - val_accuracy: 0.7027\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 1s 128ms/step - loss: 1.3237 - accuracy: 0.6870 - val_loss: 1.2091 - val_accuracy: 0.7476\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 1s 127ms/step - loss: 1.0553 - accuracy: 0.7310 - val_loss: 1.0561 - val_accuracy: 0.7607\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.9062 - accuracy: 0.7575 - val_loss: 0.9521 - val_accuracy: 0.7846\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 0.8006 - accuracy: 0.8030 - val_loss: 0.8920 - val_accuracy: 0.7943\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 1s 131ms/step - loss: 0.6277 - accuracy: 0.8245 - val_loss: 0.8243 - val_accuracy: 0.8185\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 1s 133ms/step - loss: 0.5593 - accuracy: 0.8495 - val_loss: 0.7959 - val_accuracy: 0.8279\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.5056 - accuracy: 0.8645 - val_loss: 0.7881 - val_accuracy: 0.8269\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.4365 - accuracy: 0.8810 - val_loss: 0.7591 - val_accuracy: 0.8257\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.3965 - accuracy: 0.8900 - val_loss: 0.7255 - val_accuracy: 0.8441\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 131ms/step - loss: 0.3753 - accuracy: 0.9020 - val_loss: 0.7172 - val_accuracy: 0.8438\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 128ms/step - loss: 0.3215 - accuracy: 0.9125 - val_loss: 0.7263 - val_accuracy: 0.8397\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.3119 - accuracy: 0.9105 - val_loss: 0.7282 - val_accuracy: 0.8393\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 1s 132ms/step - loss: 0.2756 - accuracy: 0.9160 - val_loss: 0.7249 - val_accuracy: 0.8431\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.2367 - accuracy: 0.9320 - val_loss: 0.7137 - val_accuracy: 0.8475\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.2943 - accuracy: 0.9185 - val_loss: 0.7206 - val_accuracy: 0.8458\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 127ms/step - loss: 0.2544 - accuracy: 0.9265 - val_loss: 0.7203 - val_accuracy: 0.8465\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 131ms/step - loss: 0.2271 - accuracy: 0.9350 - val_loss: 0.7122 - val_accuracy: 0.8475\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 128ms/step - loss: 0.2274 - accuracy: 0.9375 - val_loss: 0.7214 - val_accuracy: 0.8448\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.2175 - accuracy: 0.9325 - val_loss: 0.7222 - val_accuracy: 0.8506\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.1901 - accuracy: 0.9395 - val_loss: 0.7307 - val_accuracy: 0.8528\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 129ms/step - loss: 0.1809 - accuracy: 0.9465 - val_loss: 0.7380 - val_accuracy: 0.8509\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 1s 130ms/step - loss: 0.1505 - accuracy: 0.9505 - val_loss: 0.7367 - val_accuracy: 0.8542\n",
            "test obs. 2000 accuracy 0.8694706308919506\n",
            "test obs. 2000 b_accuracy 0.8333853032265629\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 3s 126ms/step - loss: 4.3151 - accuracy: 0.0522 - val_loss: 3.7948 - val_accuracy: 0.2644\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 2s 108ms/step - loss: 3.3673 - accuracy: 0.2550 - val_loss: 2.3749 - val_accuracy: 0.5666\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 2.2735 - accuracy: 0.4910 - val_loss: 1.2644 - val_accuracy: 0.7684\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 1.4978 - accuracy: 0.6424 - val_loss: 0.8395 - val_accuracy: 0.8131\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 1.0830 - accuracy: 0.7364 - val_loss: 0.6754 - val_accuracy: 0.8567\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 2s 107ms/step - loss: 0.8153 - accuracy: 0.7852 - val_loss: 0.5678 - val_accuracy: 0.8767\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 2s 107ms/step - loss: 0.6358 - accuracy: 0.8332 - val_loss: 0.5143 - val_accuracy: 0.8871\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 2s 107ms/step - loss: 0.5010 - accuracy: 0.8684 - val_loss: 0.4778 - val_accuracy: 0.8920\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 2s 107ms/step - loss: 0.4531 - accuracy: 0.8834 - val_loss: 0.4692 - val_accuracy: 0.8927\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 2s 107ms/step - loss: 0.4230 - accuracy: 0.8930 - val_loss: 0.4601 - val_accuracy: 0.8953\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 2s 105ms/step - loss: 0.4096 - accuracy: 0.8998 - val_loss: 0.4499 - val_accuracy: 0.9033\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 0.3185 - accuracy: 0.9048 - val_loss: 0.4319 - val_accuracy: 0.9103\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 2s 104ms/step - loss: 0.2855 - accuracy: 0.9188 - val_loss: 0.4429 - val_accuracy: 0.9091\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 0.2492 - accuracy: 0.9224 - val_loss: 0.4406 - val_accuracy: 0.9074\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 0.2643 - accuracy: 0.9272 - val_loss: 0.4409 - val_accuracy: 0.9137\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 0.2414 - accuracy: 0.9354 - val_loss: 0.4320 - val_accuracy: 0.9185\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 2s 106ms/step - loss: 0.2117 - accuracy: 0.9412 - val_loss: 0.4443 - val_accuracy: 0.9183\n",
            "test obs. 5000 accuracy 0.9311094996374184\n",
            "test obs. 5000 b_accuracy 0.8673427929455708\n",
            "Epoch 1/100\n",
            "40/40 [==============================] - 4s 107ms/step - loss: 4.0142 - accuracy: 0.1168 - val_loss: 2.6242 - val_accuracy: 0.4897\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - 4s 99ms/step - loss: 2.3006 - accuracy: 0.4882 - val_loss: 0.9551 - val_accuracy: 0.7955\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 1.3418 - accuracy: 0.6800 - val_loss: 0.5686 - val_accuracy: 0.8801\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.9121 - accuracy: 0.7803 - val_loss: 0.4514 - val_accuracy: 0.9050\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.7182 - accuracy: 0.8212 - val_loss: 0.3973 - val_accuracy: 0.9123\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.6106 - accuracy: 0.8524 - val_loss: 0.3583 - val_accuracy: 0.9258\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - 4s 97ms/step - loss: 0.5065 - accuracy: 0.8760 - val_loss: 0.3572 - val_accuracy: 0.9260\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.4335 - accuracy: 0.8923 - val_loss: 0.3400 - val_accuracy: 0.9304\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - 4s 99ms/step - loss: 0.3777 - accuracy: 0.9057 - val_loss: 0.3299 - val_accuracy: 0.9350\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.3187 - accuracy: 0.9105 - val_loss: 0.3310 - val_accuracy: 0.9374\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - 4s 97ms/step - loss: 0.2978 - accuracy: 0.9231 - val_loss: 0.3416 - val_accuracy: 0.9359\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - 4s 99ms/step - loss: 0.2408 - accuracy: 0.9339 - val_loss: 0.3444 - val_accuracy: 0.9386\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - 4s 98ms/step - loss: 0.2726 - accuracy: 0.9335 - val_loss: 0.3358 - val_accuracy: 0.9413\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - 4s 100ms/step - loss: 0.2727 - accuracy: 0.9336 - val_loss: 0.3346 - val_accuracy: 0.9451\n",
            "test obs. 10000 accuracy 0.9485134155184917\n",
            "test obs. 10000 b_accuracy 0.9202378662714499\n",
            "Epoch 1/100\n",
            "59/59 [==============================] - 6s 103ms/step - loss: 3.6382 - accuracy: 0.2100 - val_loss: 1.5404 - val_accuracy: 0.7227\n",
            "Epoch 2/100\n",
            "59/59 [==============================] - 6s 95ms/step - loss: 1.6093 - accuracy: 0.6299 - val_loss: 0.6049 - val_accuracy: 0.8714\n",
            "Epoch 3/100\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.9812 - accuracy: 0.7656 - val_loss: 0.4420 - val_accuracy: 0.9019\n",
            "Epoch 4/100\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.7182 - accuracy: 0.8274 - val_loss: 0.3602 - val_accuracy: 0.9144\n",
            "Epoch 5/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.5916 - accuracy: 0.8587 - val_loss: 0.3069 - val_accuracy: 0.9309\n",
            "Epoch 6/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.4679 - accuracy: 0.8796 - val_loss: 0.2838 - val_accuracy: 0.9430\n",
            "Epoch 7/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.3775 - accuracy: 0.9031 - val_loss: 0.2871 - val_accuracy: 0.9408\n",
            "Epoch 8/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.3421 - accuracy: 0.9123 - val_loss: 0.2799 - val_accuracy: 0.9427\n",
            "Epoch 9/100\n",
            "59/59 [==============================] - 6s 100ms/step - loss: 0.3043 - accuracy: 0.9261 - val_loss: 0.2789 - val_accuracy: 0.9425\n",
            "Epoch 10/100\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.2500 - accuracy: 0.9337 - val_loss: 0.2882 - val_accuracy: 0.9427\n",
            "Epoch 11/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.2537 - accuracy: 0.9376 - val_loss: 0.2954 - val_accuracy: 0.9434\n",
            "Epoch 12/100\n",
            "59/59 [==============================] - 6s 97ms/step - loss: 0.2415 - accuracy: 0.9395 - val_loss: 0.3167 - val_accuracy: 0.9432\n",
            "Epoch 13/100\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.2325 - accuracy: 0.9433 - val_loss: 0.3173 - val_accuracy: 0.9442\n",
            "Epoch 14/100\n",
            "59/59 [==============================] - 6s 96ms/step - loss: 0.1765 - accuracy: 0.9511 - val_loss: 0.3150 - val_accuracy: 0.9485\n",
            "test obs. 15000 accuracy 0.9564902102973168\n",
            "test obs. 15000 b_accuracy 0.9300681098699748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV-P8u5Cnt1p",
        "colab_type": "text"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcY8yFVzijpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for obs in [500,1000,2000,5000,10000,15000]:\n",
        "    run_RNN(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3UXIengmold",
        "colab_type": "text"
      },
      "source": [
        "## Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHaqJZZ2mq__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "import os, datetime\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        " \n",
        "def run_CNN_best(X_train_pad,y_train_enc,X_val_pad,y_val_enc,X_test_pad,y_test,vocab_size,embedding_matrix,class_weight_dict):\n",
        "    \n",
        "    dropout_rate=.7\n",
        "    lr = .0005\n",
        "    opt = Adam(lr=lr, decay=lr/75)\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), dtype = 'float',name='text_input')\n",
        "    embedd_seq = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = seq_len, trainable = True, mask_zero=False,name='muse_embedding')(input_layer)\n",
        "    conv_layer = Conv1D(filters=150,   kernel_size= 5,   padding='valid',  activation='relu', strides=1,name='1d_convolution')(embedd_seq)\n",
        "    pool_layer = GlobalMaxPooling1D(name='max_pooling')(conv_layer)\n",
        "    #pool_layer = GlobalAveragePooling1D()(embedd_seq)\n",
        "    drop_layer = Dropout(dropout_rate,name='dropout')(pool_layer)\n",
        "    pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax',name='dense_prediction')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "    cnn_de_fin = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "    cnn_de_fin.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "    print(cnn_de_fin.summary())\n",
        "    hist = cnn_de_fin.fit(x = X_train_pad, y = y_train_enc,\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 64, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test = cnn_de_fin.predict(X_test_pad)\n",
        "    y_pred_arg = y_pred_test.argmax(axis=1)\n",
        "    y_pred_test= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "    print('test obs.','accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.','b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i2b-lIxm5o1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_CNN_best(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpYnPpAkm5su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t6ZEVE7KRxxI"
      },
      "source": [
        "# Single Language Classifier French"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wG6eoMcYRxxY"
      },
      "source": [
        "##LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jtpfbZ1IRxxZ",
        "colab": {}
      },
      "source": [
        "stra = False\n",
        "for obs in [250,500,1000,2000,5000]:\n",
        "    tf_idf_log_reg(X_train_de,y_train_de,X_test_de,y_test_de,obs,class_weight_dict_de)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQOx-1YTRxxf"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nc-BPUUHRxxj",
        "colab": {}
      },
      "source": [
        "for obs in [500,1000,2000,5000]:\n",
        "     run_CNN(X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,vocab_size_fr,embedding_matrix_fr,class_weight_dict_fr,obs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuMmlUmAophZ",
        "colab_type": "text"
      },
      "source": [
        "## CNN2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k3B0O9lopuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000]:\n",
        "     run_CNN2D(X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,vocab_size_fr,embedding_matrix_fr,class_weight_dict_fr,obs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ak041rTCRxxl"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NWAf2EVxRxxn",
        "colab": {}
      },
      "source": [
        "for obs in [500,1000,2000,5000]:\n",
        "     run_RNN(X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,vocab_size_fr,embedding_matrix_fr,class_weight_dict_fr,obs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-j02O7MAFCg",
        "colab_type": "text"
      },
      "source": [
        "## Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM5trXPSDTQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_CNN_best(X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,vocab_size_fr,embedding_matrix_fr,class_weight_dict_fr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6K49N-YR5bA",
        "colab_type": "text"
      },
      "source": [
        "# Multilingual Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTxZorK9mM3F",
        "colab_type": "text"
      },
      "source": [
        "load embedings  \n",
        "we only load a \"slim\" version of the embeddings, which are a subset of the vocab (less than 5%) the time loading the embeddings decreases from 15 min to 7 sec and colab is capable of loading more than two languages (embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjj7J65SZxsE",
        "colab_type": "text"
      },
      "source": [
        "### Co2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvdEBWzmZ2wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install experiment-impact-tracker\n",
        "#!mkdir logs\n",
        "#from experiment_impact_tracker.compute_tracker import ImpactTracker\n",
        "#tracker = ImpactTracker('logs')\n",
        "#tracker.launch_impact_monitor()\n",
        "#info = tracker.get_latest_info_and_check_for_errors()\n",
        "#!ls logs/impacttracker\n",
        "#!cat logs/impacttracker/impact_tracker_log.log\n",
        "#!wget 'https://raw.githubusercontent.com/ELehmann91/Thesis_Multilingual_Transferlearning/master/data/model.json'\n",
        "!create-compute-appendix logs/ --site_spec model.json --output_dir logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RffXXZigrgGL",
        "colab_type": "text"
      },
      "source": [
        "### LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pATmz_dn5Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.mean(X_train_emb_de,axis=1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFXurlup_V_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_idf_log_reg(X_train,y_train,X_test,y_test,no):\n",
        "    X_train_vec = np.mean(X_train[:no],axis=1)\n",
        "    y_train = y_train[:no]\n",
        "    X_test_vec = np.mean(X_test,axis=1)\n",
        "\n",
        "    logreg = LogisticRegression(C=0.9,max_iter=100, solver='saga',penalty='elasticnet',l1_ratio=.1)\n",
        "    logreg.fit(X_train_vec, y_train)\n",
        "\n",
        "    y_pred_train = logreg.predict(X_train_vec)\n",
        "    y_pred_test = logreg.predict(X_test_vec)\n",
        "\n",
        "    print('train obs.',no,'accuracy %s' % accuracy_score(y_pred_train, y_train))\n",
        "    print('train obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_train, y_train))\n",
        "    print('test obs.',no,'accuracy %s' % accuracy_score(y_pred_test, y_test))\n",
        "    print('test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test, y_test))\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTZdxxjho_eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_log_reg(X_train_emb_de,y_train_de,X_test_emb_fr,y_test_fr,30000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSKd2GyTQsEC",
        "colab_type": "text"
      },
      "source": [
        "## CNN 1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zGvt_MZUHXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Bidirectional,LSTM, GlobalAveragePooling1D, Concatenate,Conv1D,BatchNormalization,Add,Masking,GlobalMaxPooling1D,LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.regularizers import l1,l2\n",
        "dropout_rate=.1\n",
        "\n",
        "input_layer = Input(shape = (seq_len,embedding_dim,), name='text_input')\n",
        "pool_layer = GlobalAveragePooling1D(name='avg_pooling')(input_layer)\n",
        "#dens_layer = Dense(155, activation='relu',name='dense150')(pool_layer)\n",
        "drop_layer = Dropout(dropout_rate,name='drop')(pool_layer)\n",
        "pred_layer = Dense(no_Classes, activation = 'softmax', name='output_de')(drop_layer) \n",
        "\n",
        "de_avg_pool = Model(inputs = [input_layer], outputs = [pred_layer])\n",
        "de_avg_pool.summary()\n",
        "lr = .005\n",
        "opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "de_avg_pool.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "hist = de_avg_pool.fit(x = X_train_emb_de, y = y_train_enc_de,\\\n",
        "                validation_data = (X_val_emb_de, y_val_enc_de), \\\n",
        "                epochs = 100, batch_size = 256, shuffle = True, \\\n",
        "                class_weight = class_weight_dict_de, \\\n",
        "                callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3dRH7T9qyNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,balanced_accuracy_score,confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def test_xy(X,y,string,model):\n",
        "    y_pred = model.predict([X])\n",
        "    y_pred_arg = y_pred.argmax(axis=1)\n",
        "    pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "    print('accuracy %s'% string,accuracy_score(pred,y))\n",
        "    print('b_accuracy %s'%  string,balanced_accuracy_score(pred, y))\n",
        "\n",
        "\n",
        "test_xy(X_test_emb_de,y_test_de,'german',de_avg_pool)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',de_avg_pool)\n",
        "test_xy(X_val_emb_fr,y_val_fr,'french',de_avg_pool)\n",
        "test_xy(X_train_emb_fr,y_train_fr,'french',de_avg_pool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eszN0r_mWfF9",
        "colab_type": "text"
      },
      "source": [
        "### deeper prediction analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1x9GSS-aIW",
        "colab_type": "text"
      },
      "source": [
        "Prediction / Certainty Treshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-69cpC97SWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_xy(X,y,string,model,z,ret=False):\n",
        "    obs = len(X)\n",
        "    y_pred = model.predict([X])\n",
        "    y_pred_arg = y_pred.argmax(axis=1)\n",
        "    y_pred_max = y_pred.max(axis=1)\n",
        "    pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "    df_pred = pd.DataFrame(zip(X,pred,y,y_pred_max),columns=['embed','pred','y','y_pred_max'])\n",
        "    df_pred = df_pred[df_pred['y_pred_max']>z]\n",
        "    print('accuracy %s'% string,accuracy_score(df_pred['pred'],df_pred['y']))\n",
        "    print('b_accuracy %s'%  string,balanced_accuracy_score(df_pred['pred'],df_pred['y']))\n",
        "    print(len(df_pred),'of',len(X),int(len(df_pred)/len(X)*100),'%',len(df_pred.pred.unique()))\n",
        "    if ret:\n",
        "        return df_pred\n",
        "test_xy(X_test_emb_de,y_test_de,'german',de_avg_pool,.65)\n",
        "test_xy(X_traprint(classification_report(y,pred))in_emb_fr,y_train_fr,'french',de_avg_pool,.8)\n",
        "    #return pred, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAhFro1C3m_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y,pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYl0oREB_DEd",
        "colab_type": "text"
      },
      "source": [
        "Accuracy for higher hirachies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiZ2S_iaAry2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('coicop_5_4.txt') as json_file:#\n",
        "    coicop_5_4 = json.load(json_file)\n",
        "\n",
        "with open('coicop_5_3.txt') as json_file:#\n",
        "    coicop_5_3 = json.load(json_file)\n",
        "\n",
        "y_pred = de_avg_pool.predict([X_test_emb_fr])\n",
        "y_pred_arg = y_pred.argmax(axis=1)\n",
        "pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "y_pr_lab3 = [coicop_5_3[cc5] for cc5 in pred]\n",
        "y_pr_lab4 = [coicop_5_4[cc5] for cc5 in pred]\n",
        "y_lab3 = [coicop_5_3[cc5] for cc5 in y_test_fr]\n",
        "y_lab4 = [coicop_5_4[cc5] for cc5 in y_test_fr]\n",
        "print('accuracy %s'% accuracy_score(y_pr_lab3,y_lab3))\n",
        "print('b_accuracy %s'%  balanced_accuracy_score(y_pr_lab3, y_lab3))\n",
        "print('accuracy %s'% accuracy_score(y_pr_lab4,y_lab4))\n",
        "print('b_accuracy %s'%  balanced_accuracy_score(y_pr_lab4, y_lab4))\n",
        "#print(classification_report(y_lab4,y_pr_lab4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8L2OqDn3DZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "y_pred = de_avg_pool.predict([X_test_emb_fr])\n",
        "y_pred_arg = y_pred.argmax(axis=1)\n",
        "pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "y_pr_lab4 = [coicop_5_4[cc5] for cc5 in pred]\n",
        "y_lab4 = [coicop_5_4[cc5] for cc5 in y_test_fr]\n",
        "\n",
        "label = pd.Series(y_lab4).unique()\n",
        "label.sort()\n",
        "cm = confusion_matrix(y_lab4,y_pr_lab4,labels=label)#, normalize='true')\n",
        "\n",
        "df_cm = pd.DataFrame(cm, label,label)\n",
        "plt.figure(figsize=(15,15))\n",
        "sn.set(font_scale=.7) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 9}) # font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FVqFByYZkAd",
        "colab_type": "text"
      },
      "source": [
        "### Transferlearning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRS98lLSeovB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "few_shot = 1000\n",
        "\n",
        "lr = .005\n",
        "opt = Adam(lr=lr, decay=lr/100)\n",
        "inp = de_avg_pool.input\n",
        "out = de_avg_pool.output #get_layer('output_de').output\n",
        "\n",
        "# create a new network between inp and out\n",
        "model_new = Model(inp, out)\n",
        "model_new.summary()\n",
        "model_new.get_layer('convolution').trainable = False\n",
        "model_new.get_layer('drop').rate = .5\n",
        "\n",
        "model_new.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "model_new.fit(x = X_train_emb_fr[:few_shot], y = y_train_enc_fr[:few_shot],\\\n",
        "                validation_data = (X_val_emb_fr, y_val_enc_fr), \\\n",
        "                epochs = 100, batch_size = 32, shuffle = True, \\\n",
        "                class_weight = class_weight_dict_fr, \\\n",
        "                callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiUvZ67RtYdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_xy(X_test_emb_de,y_test_de,'german',model_new)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',model_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771mi_iCz_bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "y_pred = model_new.predict([X_test_emb_fr])\n",
        "y_pred_arg = y_pred.argmax(axis=1)\n",
        "pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "y_pr_lab4 = [coicop_5_4[cc5] for cc5 in pred]\n",
        "y_lab4 = [coicop_5_4[cc5] for cc5 in y_test_fr]\n",
        "print('accuracy %s'% accuracy_score(y_pr_lab4,y_lab4))\n",
        "print('b_accuracy %s'%  balanced_accuracy_score(y_pr_lab4, y_lab4))\n",
        "\n",
        "label = pd.Series(y_test_fr).unique()\n",
        "label.sort()\n",
        "cm = confusion_matrix(y_test_fr,pred,labels=label)#, normalize='true')\n",
        "\n",
        "df_cm = pd.DataFrame(cm, label,label)\n",
        "plt.figure(figsize=(15,15))\n",
        "sn.set(font_scale=.7) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 9}) # font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZNTUwp4oGKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_xy(X_test_emb_de,y_test_de,'german',de_avg_pool_2)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',de_avg_pool_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO2e65uFcFqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(classification_report(pred, df_acc['cc5']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO1wYCsJeTkT",
        "colab_type": "text"
      },
      "source": [
        "## CCN 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VagmlBtNeV1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a91c7b9-672b-47e5-e2bd-0799a7d07013"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten,Reshape, GlobalAveragePooling1D,MaxPool2D,AvgPool2D, Concatenate,Conv1D,Conv2D,BatchNormalization,Add,Masking,GlobalMaxPooling1D,LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.regularizers import l1,l2\n",
        "dropout_rate=.35\n",
        "filter_sizes = [1,2,3,4,5]\n",
        "num_filters = 15\n",
        "\n",
        "input_layer = Input(shape = (seq_len,embedding_dim,), name='text_input')\n",
        "x = Reshape((seq_len, embedding_dim, 1))(input_layer)\n",
        "\n",
        "maxpool_pool = []\n",
        "for i in range(len(filter_sizes)):\n",
        "    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_dim),\n",
        "                                  kernel_initializer='he_normal', activation='relu')(x)\n",
        "    maxpool_pool.append(AvgPool2D(pool_size=(seq_len - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "pool_layer = Concatenate(axis=1)(maxpool_pool)   \n",
        "\n",
        "#conv_layer = Conv2D(filters=100,   kernel_size=(5,300),   padding='same',  activation='relu', strides=1,name='convolution')(x)\n",
        "#pool_layer = MaxPool2D(pool_size = (35,1),name='max_pooling')(conv_layer)\n",
        "falt_layer = Flatten(name='flat')(pool_layer)\n",
        "drop_layer = Dropout(dropout_rate,name='drop')(falt_layer)\n",
        "pred_layer = Dense(no_Classes, activation = 'softmax', name='output_de')(drop_layer) \n",
        "\n",
        "de_cnn2d = Model(inputs = [input_layer], outputs = [pred_layer])\n",
        "de_cnn2d.summary()\n",
        "lr = .005\n",
        "opt = Adam(lr=lr, decay=lr/150)\n",
        "\n",
        "de_cnn2d.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "hist = de_cnn2d.fit(x = X_train_emb_de, y = y_train_enc_de,\\\n",
        "                validation_data = (X_val_emb_de, y_val_enc_de), \\\n",
        "                epochs = 150, batch_size = 256, shuffle = True, \\\n",
        "                class_weight = class_weight_dict_de, \\\n",
        "                callbacks = [early_stopping])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_65\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "text_input (InputLayer)         [(None, 39, 300)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_22 (Reshape)            (None, 39, 300, 1)   0           text_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 39, 1, 15)    4515        reshape_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 38, 1, 15)    9015        reshape_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 37, 1, 15)    13515       reshape_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 36, 1, 15)    18015       reshape_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 35, 1, 15)    22515       reshape_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_45 (AveragePo (None, 1, 1, 15)     0           conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_46 (AveragePo (None, 1, 1, 15)     0           conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_47 (AveragePo (None, 1, 1, 15)     0           conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_48 (AveragePo (None, 1, 1, 15)     0           conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_49 (AveragePo (None, 1, 1, 15)     0           conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 5, 1, 15)     0           average_pooling2d_45[0][0]       \n",
            "                                                                 average_pooling2d_46[0][0]       \n",
            "                                                                 average_pooling2d_47[0][0]       \n",
            "                                                                 average_pooling2d_48[0][0]       \n",
            "                                                                 average_pooling2d_49[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flat (Flatten)                  (None, 75)           0           concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "drop (Dropout)                  (None, 75)           0           flat[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "output_de (Dense)               (None, 74)           5624        drop[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 73,199\n",
            "Trainable params: 73,199\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n",
            "65/65 [==============================] - 1s 15ms/step - loss: 4.0169 - accuracy: 0.1205 - val_loss: 3.3223 - val_accuracy: 0.2833\n",
            "Epoch 2/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 3.0867 - accuracy: 0.2754 - val_loss: 2.4628 - val_accuracy: 0.4784\n",
            "Epoch 3/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 2.3870 - accuracy: 0.3996 - val_loss: 1.9761 - val_accuracy: 0.5862\n",
            "Epoch 4/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 1.9455 - accuracy: 0.5013 - val_loss: 1.5873 - val_accuracy: 0.6775\n",
            "Epoch 5/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 1.6925 - accuracy: 0.5628 - val_loss: 1.3504 - val_accuracy: 0.6908\n",
            "Epoch 6/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 1.4808 - accuracy: 0.6055 - val_loss: 1.2070 - val_accuracy: 0.7252\n",
            "Epoch 7/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 1.3438 - accuracy: 0.6386 - val_loss: 1.0360 - val_accuracy: 0.7774\n",
            "Epoch 8/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 1.2196 - accuracy: 0.6811 - val_loss: 0.9253 - val_accuracy: 0.8008\n",
            "Epoch 9/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 1.1340 - accuracy: 0.6969 - val_loss: 0.8584 - val_accuracy: 0.8238\n",
            "Epoch 10/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 1.0577 - accuracy: 0.7192 - val_loss: 0.8025 - val_accuracy: 0.8337\n",
            "Epoch 11/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.9850 - accuracy: 0.7377 - val_loss: 0.7431 - val_accuracy: 0.8414\n",
            "Epoch 12/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.9315 - accuracy: 0.7534 - val_loss: 0.7014 - val_accuracy: 0.8545\n",
            "Epoch 13/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.8845 - accuracy: 0.7649 - val_loss: 0.6734 - val_accuracy: 0.8550\n",
            "Epoch 14/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.8361 - accuracy: 0.7727 - val_loss: 0.6251 - val_accuracy: 0.8765\n",
            "Epoch 15/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.7840 - accuracy: 0.7892 - val_loss: 0.5932 - val_accuracy: 0.8741\n",
            "Epoch 16/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.7665 - accuracy: 0.8017 - val_loss: 0.5708 - val_accuracy: 0.8818\n",
            "Epoch 17/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.7233 - accuracy: 0.8076 - val_loss: 0.5506 - val_accuracy: 0.8900\n",
            "Epoch 18/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.6987 - accuracy: 0.8167 - val_loss: 0.5412 - val_accuracy: 0.8818\n",
            "Epoch 19/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.6793 - accuracy: 0.8202 - val_loss: 0.5093 - val_accuracy: 0.8910\n",
            "Epoch 20/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.6455 - accuracy: 0.8310 - val_loss: 0.4998 - val_accuracy: 0.8917\n",
            "Epoch 21/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.6408 - accuracy: 0.8336 - val_loss: 0.4697 - val_accuracy: 0.8980\n",
            "Epoch 22/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.6026 - accuracy: 0.8390 - val_loss: 0.4557 - val_accuracy: 0.9007\n",
            "Epoch 23/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5954 - accuracy: 0.8405 - val_loss: 0.4410 - val_accuracy: 0.9086\n",
            "Epoch 24/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5669 - accuracy: 0.8492 - val_loss: 0.4323 - val_accuracy: 0.9057\n",
            "Epoch 25/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5476 - accuracy: 0.8538 - val_loss: 0.4175 - val_accuracy: 0.9115\n",
            "Epoch 26/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5317 - accuracy: 0.8574 - val_loss: 0.4168 - val_accuracy: 0.9098\n",
            "Epoch 27/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5210 - accuracy: 0.8629 - val_loss: 0.4131 - val_accuracy: 0.9091\n",
            "Epoch 28/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.5091 - accuracy: 0.8654 - val_loss: 0.3972 - val_accuracy: 0.9106\n",
            "Epoch 29/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4941 - accuracy: 0.8657 - val_loss: 0.3869 - val_accuracy: 0.9137\n",
            "Epoch 30/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4865 - accuracy: 0.8718 - val_loss: 0.3837 - val_accuracy: 0.9108\n",
            "Epoch 31/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4718 - accuracy: 0.8714 - val_loss: 0.3723 - val_accuracy: 0.9202\n",
            "Epoch 32/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4707 - accuracy: 0.8745 - val_loss: 0.3617 - val_accuracy: 0.9217\n",
            "Epoch 33/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4534 - accuracy: 0.8787 - val_loss: 0.3595 - val_accuracy: 0.9222\n",
            "Epoch 34/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4294 - accuracy: 0.8825 - val_loss: 0.3475 - val_accuracy: 0.9190\n",
            "Epoch 35/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.4392 - accuracy: 0.8823 - val_loss: 0.3493 - val_accuracy: 0.9231\n",
            "Epoch 36/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4189 - accuracy: 0.8858 - val_loss: 0.3438 - val_accuracy: 0.9222\n",
            "Epoch 37/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.4213 - accuracy: 0.8855 - val_loss: 0.3418 - val_accuracy: 0.9246\n",
            "Epoch 38/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3993 - accuracy: 0.8925 - val_loss: 0.3306 - val_accuracy: 0.9246\n",
            "Epoch 39/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3924 - accuracy: 0.8909 - val_loss: 0.3244 - val_accuracy: 0.9255\n",
            "Epoch 40/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3862 - accuracy: 0.8926 - val_loss: 0.3214 - val_accuracy: 0.9260\n",
            "Epoch 41/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3886 - accuracy: 0.8952 - val_loss: 0.3163 - val_accuracy: 0.9294\n",
            "Epoch 42/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3700 - accuracy: 0.8997 - val_loss: 0.3195 - val_accuracy: 0.9265\n",
            "Epoch 43/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3652 - accuracy: 0.8999 - val_loss: 0.3165 - val_accuracy: 0.9258\n",
            "Epoch 44/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3569 - accuracy: 0.9045 - val_loss: 0.3098 - val_accuracy: 0.9263\n",
            "Epoch 45/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3535 - accuracy: 0.9017 - val_loss: 0.3028 - val_accuracy: 0.9304\n",
            "Epoch 46/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3449 - accuracy: 0.9043 - val_loss: 0.2983 - val_accuracy: 0.9297\n",
            "Epoch 47/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3357 - accuracy: 0.9068 - val_loss: 0.2962 - val_accuracy: 0.9330\n",
            "Epoch 48/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3379 - accuracy: 0.9075 - val_loss: 0.3014 - val_accuracy: 0.9304\n",
            "Epoch 49/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3343 - accuracy: 0.9049 - val_loss: 0.2887 - val_accuracy: 0.9304\n",
            "Epoch 50/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3386 - accuracy: 0.9071 - val_loss: 0.2917 - val_accuracy: 0.9328\n",
            "Epoch 51/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3178 - accuracy: 0.9139 - val_loss: 0.2879 - val_accuracy: 0.9304\n",
            "Epoch 52/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3092 - accuracy: 0.9093 - val_loss: 0.2865 - val_accuracy: 0.9285\n",
            "Epoch 53/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3195 - accuracy: 0.9111 - val_loss: 0.2792 - val_accuracy: 0.9340\n",
            "Epoch 54/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3070 - accuracy: 0.9137 - val_loss: 0.2793 - val_accuracy: 0.9340\n",
            "Epoch 55/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.3067 - accuracy: 0.9134 - val_loss: 0.2841 - val_accuracy: 0.9335\n",
            "Epoch 56/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2981 - accuracy: 0.9139 - val_loss: 0.2784 - val_accuracy: 0.9306\n",
            "Epoch 57/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.3004 - accuracy: 0.9142 - val_loss: 0.2702 - val_accuracy: 0.9345\n",
            "Epoch 58/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2864 - accuracy: 0.9175 - val_loss: 0.2693 - val_accuracy: 0.9355\n",
            "Epoch 59/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2777 - accuracy: 0.9217 - val_loss: 0.2651 - val_accuracy: 0.9357\n",
            "Epoch 60/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2811 - accuracy: 0.9194 - val_loss: 0.2623 - val_accuracy: 0.9367\n",
            "Epoch 61/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2849 - accuracy: 0.9196 - val_loss: 0.2634 - val_accuracy: 0.9362\n",
            "Epoch 62/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2693 - accuracy: 0.9201 - val_loss: 0.2567 - val_accuracy: 0.9391\n",
            "Epoch 63/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2670 - accuracy: 0.9246 - val_loss: 0.2586 - val_accuracy: 0.9381\n",
            "Epoch 64/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2617 - accuracy: 0.9242 - val_loss: 0.2548 - val_accuracy: 0.9381\n",
            "Epoch 65/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2561 - accuracy: 0.9260 - val_loss: 0.2561 - val_accuracy: 0.9401\n",
            "Epoch 66/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2593 - accuracy: 0.9249 - val_loss: 0.2546 - val_accuracy: 0.9372\n",
            "Epoch 67/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2559 - accuracy: 0.9245 - val_loss: 0.2513 - val_accuracy: 0.9391\n",
            "Epoch 68/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2506 - accuracy: 0.9276 - val_loss: 0.2577 - val_accuracy: 0.9367\n",
            "Epoch 69/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2464 - accuracy: 0.9289 - val_loss: 0.2540 - val_accuracy: 0.9386\n",
            "Epoch 70/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2490 - accuracy: 0.9294 - val_loss: 0.2506 - val_accuracy: 0.9381\n",
            "Epoch 71/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2376 - accuracy: 0.9305 - val_loss: 0.2432 - val_accuracy: 0.9393\n",
            "Epoch 72/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2347 - accuracy: 0.9303 - val_loss: 0.2482 - val_accuracy: 0.9379\n",
            "Epoch 73/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2444 - accuracy: 0.9306 - val_loss: 0.2447 - val_accuracy: 0.9427\n",
            "Epoch 74/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2359 - accuracy: 0.9297 - val_loss: 0.2449 - val_accuracy: 0.9422\n",
            "Epoch 75/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2345 - accuracy: 0.9309 - val_loss: 0.2463 - val_accuracy: 0.9427\n",
            "Epoch 76/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2240 - accuracy: 0.9314 - val_loss: 0.2426 - val_accuracy: 0.9434\n",
            "Epoch 77/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2303 - accuracy: 0.9341 - val_loss: 0.2418 - val_accuracy: 0.9456\n",
            "Epoch 78/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2259 - accuracy: 0.9352 - val_loss: 0.2372 - val_accuracy: 0.9444\n",
            "Epoch 79/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2177 - accuracy: 0.9344 - val_loss: 0.2357 - val_accuracy: 0.9442\n",
            "Epoch 80/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2175 - accuracy: 0.9366 - val_loss: 0.2383 - val_accuracy: 0.9434\n",
            "Epoch 81/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2099 - accuracy: 0.9368 - val_loss: 0.2354 - val_accuracy: 0.9439\n",
            "Epoch 82/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2193 - accuracy: 0.9355 - val_loss: 0.2349 - val_accuracy: 0.9430\n",
            "Epoch 83/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2065 - accuracy: 0.9401 - val_loss: 0.2319 - val_accuracy: 0.9461\n",
            "Epoch 84/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2071 - accuracy: 0.9377 - val_loss: 0.2285 - val_accuracy: 0.9449\n",
            "Epoch 85/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2016 - accuracy: 0.9400 - val_loss: 0.2307 - val_accuracy: 0.9442\n",
            "Epoch 86/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2033 - accuracy: 0.9412 - val_loss: 0.2315 - val_accuracy: 0.9434\n",
            "Epoch 87/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2027 - accuracy: 0.9364 - val_loss: 0.2306 - val_accuracy: 0.9454\n",
            "Epoch 88/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.2016 - accuracy: 0.9407 - val_loss: 0.2279 - val_accuracy: 0.9454\n",
            "Epoch 89/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.1910 - accuracy: 0.9417 - val_loss: 0.2302 - val_accuracy: 0.9456\n",
            "Epoch 90/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1968 - accuracy: 0.9398 - val_loss: 0.2237 - val_accuracy: 0.9471\n",
            "Epoch 91/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.2008 - accuracy: 0.9389 - val_loss: 0.2278 - val_accuracy: 0.9446\n",
            "Epoch 92/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1919 - accuracy: 0.9410 - val_loss: 0.2238 - val_accuracy: 0.9461\n",
            "Epoch 93/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1839 - accuracy: 0.9441 - val_loss: 0.2240 - val_accuracy: 0.9449\n",
            "Epoch 94/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1823 - accuracy: 0.9448 - val_loss: 0.2212 - val_accuracy: 0.9461\n",
            "Epoch 95/150\n",
            "65/65 [==============================] - 1s 12ms/step - loss: 0.1834 - accuracy: 0.9440 - val_loss: 0.2216 - val_accuracy: 0.9454\n",
            "Epoch 96/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1820 - accuracy: 0.9454 - val_loss: 0.2190 - val_accuracy: 0.9468\n",
            "Epoch 97/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1836 - accuracy: 0.9446 - val_loss: 0.2193 - val_accuracy: 0.9468\n",
            "Epoch 98/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1808 - accuracy: 0.9461 - val_loss: 0.2193 - val_accuracy: 0.9459\n",
            "Epoch 99/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1760 - accuracy: 0.9456 - val_loss: 0.2162 - val_accuracy: 0.9463\n",
            "Epoch 100/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1829 - accuracy: 0.9465 - val_loss: 0.2192 - val_accuracy: 0.9478\n",
            "Epoch 101/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1728 - accuracy: 0.9460 - val_loss: 0.2146 - val_accuracy: 0.9475\n",
            "Epoch 102/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1689 - accuracy: 0.9485 - val_loss: 0.2201 - val_accuracy: 0.9478\n",
            "Epoch 103/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1690 - accuracy: 0.9491 - val_loss: 0.2173 - val_accuracy: 0.9483\n",
            "Epoch 104/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1670 - accuracy: 0.9494 - val_loss: 0.2171 - val_accuracy: 0.9480\n",
            "Epoch 105/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1712 - accuracy: 0.9485 - val_loss: 0.2176 - val_accuracy: 0.9471\n",
            "Epoch 106/150\n",
            "65/65 [==============================] - 1s 13ms/step - loss: 0.1651 - accuracy: 0.9486 - val_loss: 0.2192 - val_accuracy: 0.9475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvo_CMKtgpv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fa62e88f-4963-4cc1-8b73-08562df4ca50"
      },
      "source": [
        "def test_xy(X,y,string,model):\n",
        "    y_pred = model.predict([X])\n",
        "    y_pred_arg = y_pred.argmax(axis=1)\n",
        "    pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "    print('accuracy %s'% string,accuracy_score(pred,y))\n",
        "    print('b_accuracy %s'%  string,balanced_accuracy_score(pred, y))\n",
        "\n",
        "\n",
        "test_xy(X_test_emb_de,y_test_de,'german',de_cnn2d)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',de_cnn2d)\n",
        "test_xy(X_val_emb_fr,y_val_fr,'french',de_cnn2d)\n",
        "test_xy(X_train_emb_fr,y_train_fr,'french',de_cnn2d)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy german 0.9506889050036258\n",
            "b_accuracy german 0.8980103969752893\n",
            "accuracy french 0.3915929203539823\n",
            "b_accuracy french 0.40650398303962376\n",
            "accuracy french 0.38671586715867157\n",
            "b_accuracy french 0.44627046134971154\n",
            "accuracy french 0.39516426725729054\n",
            "b_accuracy french 0.41444340959734666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldjtfbp8p-44",
        "colab_type": "text"
      },
      "source": [
        "### Transferlearning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhuTpfU2hrWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfer_cnn(few_shot,freeze):\n",
        "\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "    inp = de_cnn2d.input\n",
        "    out = de_cnn2d.output #get_layer('output_de').output\n",
        "\n",
        "    # create a new network between inp and out\n",
        "    de_cnn2d_transfer = Model(inp, out)\n",
        "    #de_cnn2d_transfer.summary()\n",
        "    #de_cnn2d_transfer.get_layer('flat').trainable = False\n",
        "    #de_cnn2d_transfer.get_layer('drop').rate = .6\n",
        "    \n",
        "\n",
        "    de_cnn2d_transfer.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "    #freeze\n",
        "    for l , layer in enumerate(de_cnn2d_transfer.layers[:-3]):\n",
        "        de_cnn2d_transfer.layers[l].trainable = (freeze==False)\n",
        "\n",
        "    de_cnn2d_transfer.fit(x = X_train_emb_fr[:few_shot], y = y_train_enc_fr[:few_shot],\\\n",
        "                    validation_data = (X_val_emb_fr, y_val_enc_fr), \\\n",
        "                    epochs = 100, batch_size = 32, shuffle = True, \\\n",
        "                    class_weight = class_weight_dict_fr, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    print(few_shot)\n",
        "    print(test_xy(X_test_emb_de,y_test_de,'german',de_cnn2d_transfer))\n",
        "    print(test_xy(X_test_emb_fr,y_test_fr,'french test',de_cnn2d_transfer))\n",
        "    print(test_xy(X_val_emb_fr,y_val_fr,'french val',de_cnn2d_transfer))\n",
        "    print(test_xy(X_train_emb_fr,y_train_fr,'french train',de_cnn2d_transfer))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTa7r3Rmk_ox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5126e5c6-cce5-441c-b1bf-3ed9cb53bfdb"
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000]:\n",
        "    transfer_cnn(obs,True)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 234ms/step - loss: 7.7485 - accuracy: 0.2300 - val_loss: 3.9185 - val_accuracy: 0.4590\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 4.8554 - accuracy: 0.3300 - val_loss: 3.7313 - val_accuracy: 0.4590\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 3.5603 - accuracy: 0.3900 - val_loss: 3.6592 - val_accuracy: 0.4524\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 3.3046 - accuracy: 0.3900 - val_loss: 3.6225 - val_accuracy: 0.4465\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 2.4936 - accuracy: 0.4800 - val_loss: 3.6462 - val_accuracy: 0.4384\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 2.8111 - accuracy: 0.4500 - val_loss: 3.6557 - val_accuracy: 0.4354\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 2.3242 - accuracy: 0.4800 - val_loss: 3.6275 - val_accuracy: 0.4369\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 2.2760 - accuracy: 0.4800 - val_loss: 3.5948 - val_accuracy: 0.4428\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.9144 - accuracy: 0.4800 - val_loss: 3.5663 - val_accuracy: 0.4450\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 2.1380 - accuracy: 0.5700 - val_loss: 3.5881 - val_accuracy: 0.4428\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.8727 - accuracy: 0.5400 - val_loss: 3.5169 - val_accuracy: 0.4413\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.4436 - accuracy: 0.5600 - val_loss: 3.4030 - val_accuracy: 0.4443\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.4871 - accuracy: 0.5600 - val_loss: 3.3770 - val_accuracy: 0.4421\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 1.3147 - accuracy: 0.6100 - val_loss: 3.3821 - val_accuracy: 0.4354\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 1.5342 - accuracy: 0.6000 - val_loss: 3.3627 - val_accuracy: 0.4413\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.2618 - accuracy: 0.5700 - val_loss: 3.3295 - val_accuracy: 0.4494\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 1.0162 - accuracy: 0.6400 - val_loss: 3.2914 - val_accuracy: 0.4517\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.3218 - accuracy: 0.6300 - val_loss: 3.2727 - val_accuracy: 0.4561\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1.4263 - accuracy: 0.6200 - val_loss: 3.2674 - val_accuracy: 0.4598\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.0182 - accuracy: 0.6500 - val_loss: 3.2652 - val_accuracy: 0.4554\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.9954 - accuracy: 0.6200 - val_loss: 3.2634 - val_accuracy: 0.4583\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.0773 - accuracy: 0.6600 - val_loss: 3.2403 - val_accuracy: 0.4613\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.8545 - accuracy: 0.6400 - val_loss: 3.2110 - val_accuracy: 0.4590\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.8344 - accuracy: 0.6300 - val_loss: 3.1816 - val_accuracy: 0.4635\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 0.9311 - accuracy: 0.6600 - val_loss: 3.1840 - val_accuracy: 0.4649\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.9344 - accuracy: 0.6500 - val_loss: 3.1788 - val_accuracy: 0.4664\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.8135 - accuracy: 0.6500 - val_loss: 3.1698 - val_accuracy: 0.4679\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.9433 - accuracy: 0.5800 - val_loss: 3.1460 - val_accuracy: 0.4694\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.7930 - accuracy: 0.6300 - val_loss: 3.1148 - val_accuracy: 0.4745\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.6189 - accuracy: 0.7000 - val_loss: 3.0780 - val_accuracy: 0.4782\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.5879 - accuracy: 0.7600 - val_loss: 3.0455 - val_accuracy: 0.4804\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.6478 - accuracy: 0.7200 - val_loss: 3.0284 - val_accuracy: 0.4819\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.7531 - accuracy: 0.6800 - val_loss: 3.0316 - val_accuracy: 0.4834\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 0.7248 - accuracy: 0.6800 - val_loss: 3.0357 - val_accuracy: 0.4812\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.5655 - accuracy: 0.7700 - val_loss: 3.0379 - val_accuracy: 0.4893\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.5660 - accuracy: 0.7200 - val_loss: 3.0302 - val_accuracy: 0.4952\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.6563 - accuracy: 0.7600 - val_loss: 3.0129 - val_accuracy: 0.4982\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.4776 - accuracy: 0.7400 - val_loss: 3.0086 - val_accuracy: 0.4959\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 0.4797 - accuracy: 0.7800 - val_loss: 3.0028 - val_accuracy: 0.4982\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 0.5378 - accuracy: 0.7800 - val_loss: 3.0056 - val_accuracy: 0.4967\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.5458 - accuracy: 0.7500 - val_loss: 3.0226 - val_accuracy: 0.4974\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 0.4467 - accuracy: 0.8100 - val_loss: 3.0538 - val_accuracy: 0.4900\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.3861 - accuracy: 0.8300 - val_loss: 3.0737 - val_accuracy: 0.4878\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 0.4121 - accuracy: 0.8000 - val_loss: 3.0804 - val_accuracy: 0.4900\n",
            "100\n",
            "accuracy german 0.8854242204496011\n",
            "b_accuracy german 0.8732166263354258\n",
            "None\n",
            "accuracy french test 0.5221238938053098\n",
            "b_accuracy french test 0.5117800247186213\n",
            "None\n",
            "accuracy french val 0.4900369003690037\n",
            "b_accuracy french val 0.5482298231039465\n",
            "None\n",
            "accuracy french train 0.5020302694721299\n",
            "b_accuracy french train 0.5378499204767113\n",
            "None\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 53ms/step - loss: 3.5573 - accuracy: 0.5360 - val_loss: 2.9149 - val_accuracy: 0.5085\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 3.5840 - accuracy: 0.5360 - val_loss: 2.7656 - val_accuracy: 0.5218\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 3.3248 - accuracy: 0.5840 - val_loss: 2.6470 - val_accuracy: 0.5343\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 3.4044 - accuracy: 0.5360 - val_loss: 2.5595 - val_accuracy: 0.5395\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 3.6556 - accuracy: 0.5320 - val_loss: 2.4943 - val_accuracy: 0.5424\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 2.6660 - accuracy: 0.5160 - val_loss: 2.4371 - val_accuracy: 0.5446\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.8804 - accuracy: 0.5560 - val_loss: 2.3848 - val_accuracy: 0.5506\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.6747 - accuracy: 0.5520 - val_loss: 2.3513 - val_accuracy: 0.5579\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 2.2498 - accuracy: 0.5560 - val_loss: 2.3286 - val_accuracy: 0.5616\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 2.5179 - accuracy: 0.5600 - val_loss: 2.3186 - val_accuracy: 0.5668\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 2.5265 - accuracy: 0.5240 - val_loss: 2.3099 - val_accuracy: 0.5616\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 2.1210 - accuracy: 0.5160 - val_loss: 2.2899 - val_accuracy: 0.5616\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.2317 - accuracy: 0.5640 - val_loss: 2.2787 - val_accuracy: 0.5624\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 2.2314 - accuracy: 0.5440 - val_loss: 2.2621 - val_accuracy: 0.5638\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 2.1941 - accuracy: 0.5240 - val_loss: 2.2454 - val_accuracy: 0.5638\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.2572 - accuracy: 0.5520 - val_loss: 2.2361 - val_accuracy: 0.5653\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.9550 - accuracy: 0.5520 - val_loss: 2.2411 - val_accuracy: 0.5653\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.0625 - accuracy: 0.5840 - val_loss: 2.2451 - val_accuracy: 0.5624\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.8282 - accuracy: 0.5560 - val_loss: 2.2436 - val_accuracy: 0.5690\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.1058 - accuracy: 0.5480 - val_loss: 2.2331 - val_accuracy: 0.5668\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.7082 - accuracy: 0.5400 - val_loss: 2.2263 - val_accuracy: 0.5734\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.8005 - accuracy: 0.5800 - val_loss: 2.2150 - val_accuracy: 0.5742\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4486 - accuracy: 0.5880 - val_loss: 2.2024 - val_accuracy: 0.5771\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.8103 - accuracy: 0.5680 - val_loss: 2.1978 - val_accuracy: 0.5801\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.8805 - accuracy: 0.5720 - val_loss: 2.1904 - val_accuracy: 0.5808\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.6544 - accuracy: 0.5960 - val_loss: 2.1783 - val_accuracy: 0.5860\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.7536 - accuracy: 0.5720 - val_loss: 2.1744 - val_accuracy: 0.5860\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.9264 - accuracy: 0.5800 - val_loss: 2.1726 - val_accuracy: 0.5845\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.5356 - accuracy: 0.6000 - val_loss: 2.1719 - val_accuracy: 0.5860\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.5247 - accuracy: 0.6360 - val_loss: 2.1715 - val_accuracy: 0.5852\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.5541 - accuracy: 0.5920 - val_loss: 2.1702 - val_accuracy: 0.5845\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4480 - accuracy: 0.5600 - val_loss: 2.1755 - val_accuracy: 0.5845\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4271 - accuracy: 0.6000 - val_loss: 2.1654 - val_accuracy: 0.5904\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.3749 - accuracy: 0.5960 - val_loss: 2.1533 - val_accuracy: 0.5978\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.3115 - accuracy: 0.6200 - val_loss: 2.1494 - val_accuracy: 0.5963\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.7395 - accuracy: 0.5560 - val_loss: 2.1518 - val_accuracy: 0.5970\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.8165 - accuracy: 0.5520 - val_loss: 2.1489 - val_accuracy: 0.6015\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.6312 - accuracy: 0.5920 - val_loss: 2.1530 - val_accuracy: 0.6007\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.4532 - accuracy: 0.5880 - val_loss: 2.1553 - val_accuracy: 0.6037\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.3419 - accuracy: 0.5920 - val_loss: 2.1618 - val_accuracy: 0.5970\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.2933 - accuracy: 0.6600 - val_loss: 2.1623 - val_accuracy: 0.5963\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.2517 - accuracy: 0.6600 - val_loss: 2.1535 - val_accuracy: 0.5993\n",
            "250\n",
            "accuracy german 0.8469905728788978\n",
            "b_accuracy german 0.8359651540822075\n",
            "None\n",
            "accuracy french test 0.6261061946902655\n",
            "b_accuracy french test 0.5921311246086359\n",
            "None\n",
            "accuracy french val 0.5992619926199262\n",
            "b_accuracy french val 0.5204150408777549\n",
            "None\n",
            "accuracy french train 0.6312292358803987\n",
            "b_accuracy french train 0.5523268050760193\n",
            "None\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 4.9418 - accuracy: 0.5440 - val_loss: 2.0758 - val_accuracy: 0.6066\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 4.0840 - accuracy: 0.5420 - val_loss: 2.0339 - val_accuracy: 0.5985\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 4.2334 - accuracy: 0.5760 - val_loss: 1.9821 - val_accuracy: 0.6207\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 4.0199 - accuracy: 0.5740 - val_loss: 1.9313 - val_accuracy: 0.6251\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.7017 - accuracy: 0.5240 - val_loss: 1.9103 - val_accuracy: 0.6170\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.7323 - accuracy: 0.5320 - val_loss: 1.8808 - val_accuracy: 0.6207\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.1857 - accuracy: 0.5740 - val_loss: 1.8438 - val_accuracy: 0.6162\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.1305 - accuracy: 0.5820 - val_loss: 1.8268 - val_accuracy: 0.6155\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.2891 - accuracy: 0.5680 - val_loss: 1.8071 - val_accuracy: 0.6140\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 3.1223 - accuracy: 0.5500 - val_loss: 1.7712 - val_accuracy: 0.6236\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.9607 - accuracy: 0.5980 - val_loss: 1.7458 - val_accuracy: 0.6251\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 2.7318 - accuracy: 0.5960 - val_loss: 1.7228 - val_accuracy: 0.6258\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.5820 - accuracy: 0.6120 - val_loss: 1.7023 - val_accuracy: 0.6251\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.5249 - accuracy: 0.6020 - val_loss: 1.6972 - val_accuracy: 0.6325\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 2.7882 - accuracy: 0.5900 - val_loss: 1.6889 - val_accuracy: 0.6310\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.4381 - accuracy: 0.5940 - val_loss: 1.6778 - val_accuracy: 0.6332\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.3840 - accuracy: 0.5780 - val_loss: 1.6633 - val_accuracy: 0.6317\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.8283 - accuracy: 0.5960 - val_loss: 1.6539 - val_accuracy: 0.6339\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.0437 - accuracy: 0.6100 - val_loss: 1.6529 - val_accuracy: 0.6295\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 2.1202 - accuracy: 0.6120 - val_loss: 1.6595 - val_accuracy: 0.6258\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.9163 - accuracy: 0.6060 - val_loss: 1.6621 - val_accuracy: 0.6339\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.7397 - accuracy: 0.6120 - val_loss: 1.6573 - val_accuracy: 0.6369\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.8152 - accuracy: 0.6260 - val_loss: 1.6571 - val_accuracy: 0.6428\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.5871 - accuracy: 0.6180 - val_loss: 1.6501 - val_accuracy: 0.6376\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.6360 - accuracy: 0.6080 - val_loss: 1.6449 - val_accuracy: 0.6354\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 2.1398 - accuracy: 0.5980 - val_loss: 1.6307 - val_accuracy: 0.6347\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.9957 - accuracy: 0.6200 - val_loss: 1.6184 - val_accuracy: 0.6384\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.4937 - accuracy: 0.6360 - val_loss: 1.6171 - val_accuracy: 0.6339\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.7754 - accuracy: 0.6420 - val_loss: 1.6139 - val_accuracy: 0.6354\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.7099 - accuracy: 0.6300 - val_loss: 1.6231 - val_accuracy: 0.6391\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.5784 - accuracy: 0.6000 - val_loss: 1.6176 - val_accuracy: 0.6354\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.4951 - accuracy: 0.6440 - val_loss: 1.6034 - val_accuracy: 0.6332\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.6328 - accuracy: 0.6160 - val_loss: 1.6018 - val_accuracy: 0.6391\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.4718 - accuracy: 0.6480 - val_loss: 1.6004 - val_accuracy: 0.6384\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.6846 - accuracy: 0.6300 - val_loss: 1.5963 - val_accuracy: 0.6458\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.5962 - accuracy: 0.6380 - val_loss: 1.5918 - val_accuracy: 0.6487\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.9005 - accuracy: 0.6120 - val_loss: 1.5897 - val_accuracy: 0.6517\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.2767 - accuracy: 0.6620 - val_loss: 1.5974 - val_accuracy: 0.6502\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.6115 - accuracy: 0.6100 - val_loss: 1.5907 - val_accuracy: 0.6546\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.6331 - accuracy: 0.6120 - val_loss: 1.5968 - val_accuracy: 0.6494\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.4660 - accuracy: 0.6620 - val_loss: 1.5860 - val_accuracy: 0.6517\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.7746 - accuracy: 0.6520 - val_loss: 1.5902 - val_accuracy: 0.6487\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.5554 - accuracy: 0.6200 - val_loss: 1.5982 - val_accuracy: 0.6391\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.4096 - accuracy: 0.6580 - val_loss: 1.5851 - val_accuracy: 0.6406\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.4176 - accuracy: 0.6500 - val_loss: 1.5794 - val_accuracy: 0.6391\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.5419 - accuracy: 0.6360 - val_loss: 1.5891 - val_accuracy: 0.6347\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.5519 - accuracy: 0.6260 - val_loss: 1.5776 - val_accuracy: 0.6413\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.6075 - accuracy: 0.6240 - val_loss: 1.5829 - val_accuracy: 0.6339\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.1984 - accuracy: 0.6540 - val_loss: 1.5740 - val_accuracy: 0.6391\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.1148 - accuracy: 0.6580 - val_loss: 1.5761 - val_accuracy: 0.6450\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.4530 - accuracy: 0.6360 - val_loss: 1.5730 - val_accuracy: 0.6428\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.2595 - accuracy: 0.6780 - val_loss: 1.5589 - val_accuracy: 0.6494\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.4096 - accuracy: 0.6220 - val_loss: 1.5530 - val_accuracy: 0.6502\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.4325 - accuracy: 0.6540 - val_loss: 1.5464 - val_accuracy: 0.6583\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.2096 - accuracy: 0.6660 - val_loss: 1.5644 - val_accuracy: 0.6494\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.3196 - accuracy: 0.6700 - val_loss: 1.5679 - val_accuracy: 0.6443\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 1.2152 - accuracy: 0.6620 - val_loss: 1.5647 - val_accuracy: 0.6391\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.0610 - accuracy: 0.6540 - val_loss: 1.5522 - val_accuracy: 0.6443\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.1839 - accuracy: 0.6900 - val_loss: 1.5489 - val_accuracy: 0.6472\n",
            "500\n",
            "accuracy german 0.7860768672951414\n",
            "b_accuracy german 0.7818467613492369\n",
            "None\n",
            "accuracy french test 0.7123893805309734\n",
            "b_accuracy french test 0.6447034489756409\n",
            "None\n",
            "accuracy french val 0.6472324723247233\n",
            "b_accuracy french val 0.624694886056247\n",
            "None\n",
            "accuracy french train 0.6758951642672573\n",
            "b_accuracy french train 0.6307860108104072\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.0087 - accuracy: 0.6330 - val_loss: 1.4958 - val_accuracy: 0.6672\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.8853 - accuracy: 0.6400 - val_loss: 1.4751 - val_accuracy: 0.6635\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.8461 - accuracy: 0.6410 - val_loss: 1.4633 - val_accuracy: 0.6642\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.7334 - accuracy: 0.6250 - val_loss: 1.4469 - val_accuracy: 0.6679\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.8365 - accuracy: 0.6240 - val_loss: 1.4312 - val_accuracy: 0.6849\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.6495 - accuracy: 0.6220 - val_loss: 1.4355 - val_accuracy: 0.6775\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.6115 - accuracy: 0.6460 - val_loss: 1.4293 - val_accuracy: 0.6701\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5145 - accuracy: 0.6650 - val_loss: 1.4062 - val_accuracy: 0.6775\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5883 - accuracy: 0.6480 - val_loss: 1.4143 - val_accuracy: 0.6620\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5583 - accuracy: 0.6460 - val_loss: 1.3991 - val_accuracy: 0.6738\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3027 - accuracy: 0.6540 - val_loss: 1.3970 - val_accuracy: 0.6782\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5313 - accuracy: 0.6570 - val_loss: 1.3951 - val_accuracy: 0.6819\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2868 - accuracy: 0.6740 - val_loss: 1.3822 - val_accuracy: 0.6804\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3058 - accuracy: 0.6670 - val_loss: 1.3599 - val_accuracy: 0.6841\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3347 - accuracy: 0.6550 - val_loss: 1.3596 - val_accuracy: 0.6945\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3806 - accuracy: 0.6630 - val_loss: 1.3587 - val_accuracy: 0.6856\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.3171 - accuracy: 0.6500 - val_loss: 1.3684 - val_accuracy: 0.6856\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1585 - accuracy: 0.6720 - val_loss: 1.3477 - val_accuracy: 0.6930\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1755 - accuracy: 0.6650 - val_loss: 1.3415 - val_accuracy: 0.6893\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1610 - accuracy: 0.6680 - val_loss: 1.3427 - val_accuracy: 0.6923\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2026 - accuracy: 0.6690 - val_loss: 1.3315 - val_accuracy: 0.6959\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1187 - accuracy: 0.6680 - val_loss: 1.3429 - val_accuracy: 0.6900\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1565 - accuracy: 0.6960 - val_loss: 1.3290 - val_accuracy: 0.6974\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1652 - accuracy: 0.6860 - val_loss: 1.3160 - val_accuracy: 0.6996\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.3359 - accuracy: 0.6830 - val_loss: 1.3162 - val_accuracy: 0.6996\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1870 - accuracy: 0.6730 - val_loss: 1.3231 - val_accuracy: 0.6937\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1983 - accuracy: 0.6750 - val_loss: 1.3112 - val_accuracy: 0.6989\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1309 - accuracy: 0.6760 - val_loss: 1.3254 - val_accuracy: 0.6930\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2886 - accuracy: 0.6650 - val_loss: 1.3337 - val_accuracy: 0.6900\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1291 - accuracy: 0.6910 - val_loss: 1.3328 - val_accuracy: 0.6878\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0158 - accuracy: 0.6920 - val_loss: 1.3204 - val_accuracy: 0.6974\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0365 - accuracy: 0.7190 - val_loss: 1.3023 - val_accuracy: 0.7063\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1796 - accuracy: 0.6900 - val_loss: 1.3111 - val_accuracy: 0.6952\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.2969 - accuracy: 0.6850 - val_loss: 1.3154 - val_accuracy: 0.7004\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.1472 - accuracy: 0.6830 - val_loss: 1.3162 - val_accuracy: 0.6967\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0752 - accuracy: 0.6860 - val_loss: 1.3111 - val_accuracy: 0.6996\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.0758 - accuracy: 0.6950 - val_loss: 1.3146 - val_accuracy: 0.7018\n",
            "1000\n",
            "accuracy german 0.7425670775924583\n",
            "b_accuracy german 0.7374352392237208\n",
            "None\n",
            "accuracy french test 0.745575221238938\n",
            "b_accuracy french test 0.6886448439706585\n",
            "None\n",
            "accuracy french val 0.7018450184501845\n",
            "b_accuracy french val 0.6535496926634506\n",
            "None\n",
            "accuracy french train 0.7377260981912145\n",
            "b_accuracy french train 0.6825569200971966\n",
            "None\n",
            "Epoch 1/100\n",
            "63/63 [==============================] - 1s 10ms/step - loss: 1.6300 - accuracy: 0.6605 - val_loss: 1.2879 - val_accuracy: 0.6959\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.5656 - accuracy: 0.6540 - val_loss: 1.2742 - val_accuracy: 0.7100\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.4861 - accuracy: 0.6565 - val_loss: 1.2763 - val_accuracy: 0.7041\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.4421 - accuracy: 0.6605 - val_loss: 1.2491 - val_accuracy: 0.7004\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.4870 - accuracy: 0.6600 - val_loss: 1.2563 - val_accuracy: 0.6989\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.4060 - accuracy: 0.6645 - val_loss: 1.2226 - val_accuracy: 0.7092\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.4187 - accuracy: 0.6510 - val_loss: 1.2208 - val_accuracy: 0.7070\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.3707 - accuracy: 0.6650 - val_loss: 1.2108 - val_accuracy: 0.7070\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2590 - accuracy: 0.6700 - val_loss: 1.2315 - val_accuracy: 0.7011\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.3761 - accuracy: 0.6645 - val_loss: 1.2144 - val_accuracy: 0.7018\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2224 - accuracy: 0.6690 - val_loss: 1.2045 - val_accuracy: 0.7063\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.3355 - accuracy: 0.6765 - val_loss: 1.2002 - val_accuracy: 0.7114\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2348 - accuracy: 0.6805 - val_loss: 1.2001 - val_accuracy: 0.7100\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.1978 - accuracy: 0.6740 - val_loss: 1.1797 - val_accuracy: 0.7159\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.1962 - accuracy: 0.6790 - val_loss: 1.1801 - val_accuracy: 0.7129\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2393 - accuracy: 0.6750 - val_loss: 1.1853 - val_accuracy: 0.7092\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2298 - accuracy: 0.6705 - val_loss: 1.1742 - val_accuracy: 0.7122\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.1194 - accuracy: 0.6845 - val_loss: 1.1804 - val_accuracy: 0.7070\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.1967 - accuracy: 0.6765 - val_loss: 1.1814 - val_accuracy: 0.7077\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.2217 - accuracy: 0.6805 - val_loss: 1.1572 - val_accuracy: 0.7144\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.0656 - accuracy: 0.6865 - val_loss: 1.1530 - val_accuracy: 0.7196\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.1139 - accuracy: 0.6860 - val_loss: 1.1573 - val_accuracy: 0.7210\n",
            "Epoch 23/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.1443 - accuracy: 0.6940 - val_loss: 1.1438 - val_accuracy: 0.7114\n",
            "Epoch 24/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.1519 - accuracy: 0.6765 - val_loss: 1.1525 - val_accuracy: 0.7225\n",
            "Epoch 25/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.1511 - accuracy: 0.6805 - val_loss: 1.1563 - val_accuracy: 0.7107\n",
            "Epoch 26/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0734 - accuracy: 0.6925 - val_loss: 1.1495 - val_accuracy: 0.7173\n",
            "Epoch 27/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0523 - accuracy: 0.6935 - val_loss: 1.1474 - val_accuracy: 0.7173\n",
            "Epoch 28/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0666 - accuracy: 0.6940 - val_loss: 1.1373 - val_accuracy: 0.7188\n",
            "Epoch 29/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0408 - accuracy: 0.6880 - val_loss: 1.1346 - val_accuracy: 0.7188\n",
            "Epoch 30/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0351 - accuracy: 0.7000 - val_loss: 1.1314 - val_accuracy: 0.7114\n",
            "Epoch 31/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0608 - accuracy: 0.6885 - val_loss: 1.1293 - val_accuracy: 0.7232\n",
            "Epoch 32/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.9393 - accuracy: 0.7105 - val_loss: 1.1258 - val_accuracy: 0.7262\n",
            "Epoch 33/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.9730 - accuracy: 0.7000 - val_loss: 1.0979 - val_accuracy: 0.7255\n",
            "Epoch 34/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 1.0397 - accuracy: 0.6880 - val_loss: 1.1077 - val_accuracy: 0.7292\n",
            "Epoch 35/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.9841 - accuracy: 0.7125 - val_loss: 1.1062 - val_accuracy: 0.7262\n",
            "Epoch 36/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.0311 - accuracy: 0.6980 - val_loss: 1.1133 - val_accuracy: 0.7314\n",
            "Epoch 37/100\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 1.0187 - accuracy: 0.6945 - val_loss: 1.1169 - val_accuracy: 0.7277\n",
            "Epoch 38/100\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.9625 - accuracy: 0.6960 - val_loss: 1.1170 - val_accuracy: 0.7247\n",
            "2000\n",
            "accuracy german 0.6700507614213198\n",
            "b_accuracy german 0.6432203816193502\n",
            "None\n",
            "accuracy french test 0.7610619469026548\n",
            "b_accuracy french test 0.7032055594791033\n",
            "None\n",
            "accuracy french val 0.7247232472324723\n",
            "b_accuracy french val 0.6722587598973913\n",
            "None\n",
            "accuracy french train 0.7779623477297896\n",
            "b_accuracy french train 0.7411053234574058\n",
            "None\n",
            "Epoch 1/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 1.4362 - accuracy: 0.6780 - val_loss: 1.1044 - val_accuracy: 0.7232\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.3536 - accuracy: 0.6736 - val_loss: 1.0787 - val_accuracy: 0.7410\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.4008 - accuracy: 0.6638 - val_loss: 1.0797 - val_accuracy: 0.7351\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.3372 - accuracy: 0.6788 - val_loss: 1.0688 - val_accuracy: 0.7373\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.3297 - accuracy: 0.6714 - val_loss: 1.0533 - val_accuracy: 0.7328\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.2403 - accuracy: 0.6706 - val_loss: 1.0401 - val_accuracy: 0.7365\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1983 - accuracy: 0.6888 - val_loss: 1.0191 - val_accuracy: 0.7439\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.2059 - accuracy: 0.6820 - val_loss: 1.0416 - val_accuracy: 0.7373\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.2075 - accuracy: 0.6836 - val_loss: 1.0053 - val_accuracy: 0.7439\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.2036 - accuracy: 0.6852 - val_loss: 1.0127 - val_accuracy: 0.7461\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1820 - accuracy: 0.6828 - val_loss: 1.0083 - val_accuracy: 0.7417\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1532 - accuracy: 0.6922 - val_loss: 1.0037 - val_accuracy: 0.7432\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1248 - accuracy: 0.6968 - val_loss: 1.0065 - val_accuracy: 0.7373\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1067 - accuracy: 0.6876 - val_loss: 0.9784 - val_accuracy: 0.7476\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1498 - accuracy: 0.6896 - val_loss: 0.9833 - val_accuracy: 0.7446\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.1402 - accuracy: 0.6890 - val_loss: 0.9746 - val_accuracy: 0.7528\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0505 - accuracy: 0.7006 - val_loss: 0.9760 - val_accuracy: 0.7550\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0875 - accuracy: 0.7010 - val_loss: 0.9710 - val_accuracy: 0.7491\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0779 - accuracy: 0.7022 - val_loss: 0.9676 - val_accuracy: 0.7535\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0135 - accuracy: 0.7022 - val_loss: 0.9661 - val_accuracy: 0.7520\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0613 - accuracy: 0.6978 - val_loss: 0.9661 - val_accuracy: 0.7520\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0715 - accuracy: 0.6970 - val_loss: 0.9595 - val_accuracy: 0.7528\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0703 - accuracy: 0.6992 - val_loss: 0.9597 - val_accuracy: 0.7572\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0232 - accuracy: 0.7004 - val_loss: 0.9487 - val_accuracy: 0.7557\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0069 - accuracy: 0.7066 - val_loss: 0.9461 - val_accuracy: 0.7594\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0261 - accuracy: 0.6954 - val_loss: 0.9449 - val_accuracy: 0.7550\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0448 - accuracy: 0.7030 - val_loss: 0.9522 - val_accuracy: 0.7513\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0330 - accuracy: 0.7004 - val_loss: 0.9287 - val_accuracy: 0.7646\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0133 - accuracy: 0.7022 - val_loss: 0.9221 - val_accuracy: 0.7712\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0107 - accuracy: 0.7036 - val_loss: 0.9276 - val_accuracy: 0.7565\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0045 - accuracy: 0.7080 - val_loss: 0.9349 - val_accuracy: 0.7601\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.9764 - accuracy: 0.7036 - val_loss: 0.9391 - val_accuracy: 0.7579\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.9765 - accuracy: 0.7158 - val_loss: 0.9356 - val_accuracy: 0.7587\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 1.0099 - accuracy: 0.7044 - val_loss: 0.9330 - val_accuracy: 0.7528\n",
            "5000\n",
            "accuracy german 0.5228426395939086\n",
            "b_accuracy german 0.5388941902562583\n",
            "None\n",
            "accuracy french test 0.7986725663716814\n",
            "b_accuracy french test 0.7700569203589047\n",
            "None\n",
            "accuracy french val 0.7527675276752768\n",
            "b_accuracy french val 0.7104206132070917\n",
            "None\n",
            "accuracy french train 0.8194905869324474\n",
            "b_accuracy french train 0.7766273334619698\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjowWaogDpDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9da42f1c-f606-4fee-95b4-81f2d6c67ba8"
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000]:\n",
        "    transfer_cnn(obs,False)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 0s 101ms/step - loss: 10.6136 - accuracy: 0.3000 - val_loss: 4.5651 - val_accuracy: 0.3934\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 7.7709 - accuracy: 0.2900 - val_loss: 4.1246 - val_accuracy: 0.4052\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 5.7543 - accuracy: 0.2700 - val_loss: 3.7888 - val_accuracy: 0.4170\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 6.3170 - accuracy: 0.2700 - val_loss: 3.5374 - val_accuracy: 0.4185\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 3.7700 - accuracy: 0.3500 - val_loss: 3.3438 - val_accuracy: 0.4177\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 3.8244 - accuracy: 0.3200 - val_loss: 3.2043 - val_accuracy: 0.4162\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 1s 178ms/step - loss: 3.9208 - accuracy: 0.4000 - val_loss: 3.1236 - val_accuracy: 0.4162\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 2.8570 - accuracy: 0.3400 - val_loss: 3.0707 - val_accuracy: 0.4170\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 2.8494 - accuracy: 0.3900 - val_loss: 3.0457 - val_accuracy: 0.4148\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 2.5618 - accuracy: 0.4100 - val_loss: 3.0313 - val_accuracy: 0.4133\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 2.3226 - accuracy: 0.4200 - val_loss: 3.0170 - val_accuracy: 0.4177\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 2.7528 - accuracy: 0.3800 - val_loss: 2.9966 - val_accuracy: 0.4207\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 1.9370 - accuracy: 0.4700 - val_loss: 2.9764 - val_accuracy: 0.4207\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 2.2126 - accuracy: 0.4900 - val_loss: 2.9586 - val_accuracy: 0.4192\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 2.0138 - accuracy: 0.4600 - val_loss: 2.9318 - val_accuracy: 0.4207\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1.8940 - accuracy: 0.5100 - val_loss: 2.9121 - val_accuracy: 0.4192\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.4832 - accuracy: 0.5300 - val_loss: 2.9017 - val_accuracy: 0.4236\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 2.1999 - accuracy: 0.4800 - val_loss: 2.8957 - val_accuracy: 0.4177\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1.6005 - accuracy: 0.5100 - val_loss: 2.8707 - val_accuracy: 0.4199\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.4061 - accuracy: 0.5200 - val_loss: 2.8436 - val_accuracy: 0.4273\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 1.3285 - accuracy: 0.5500 - val_loss: 2.8295 - val_accuracy: 0.4332\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 1.2927 - accuracy: 0.5600 - val_loss: 2.8270 - val_accuracy: 0.4310\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1.2769 - accuracy: 0.5900 - val_loss: 2.8304 - val_accuracy: 0.4310\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 1.1541 - accuracy: 0.6000 - val_loss: 2.8326 - val_accuracy: 0.4347\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.0179 - accuracy: 0.6200 - val_loss: 2.8345 - val_accuracy: 0.4347\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.5271 - accuracy: 0.5200 - val_loss: 2.8236 - val_accuracy: 0.4332\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 1.1381 - accuracy: 0.5800 - val_loss: 2.8179 - val_accuracy: 0.4303\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 1.0545 - accuracy: 0.5900 - val_loss: 2.8152 - val_accuracy: 0.4376\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.0200 - accuracy: 0.6400 - val_loss: 2.8125 - val_accuracy: 0.4487\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.0039 - accuracy: 0.5900 - val_loss: 2.8134 - val_accuracy: 0.4539\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.0060 - accuracy: 0.5600 - val_loss: 2.8245 - val_accuracy: 0.4554\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 1.0268 - accuracy: 0.6000 - val_loss: 2.8279 - val_accuracy: 0.4605\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1.0498 - accuracy: 0.5900 - val_loss: 2.8349 - val_accuracy: 0.4598\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1.0255 - accuracy: 0.5600 - val_loss: 2.8450 - val_accuracy: 0.4554\n",
            "100\n",
            "accuracy german 0.8934010152284264\n",
            "b_accuracy german 0.8742084920243699\n",
            "None\n",
            "accuracy french test 0.45132743362831856\n",
            "b_accuracy french test 0.45777078013261596\n",
            "None\n",
            "accuracy french val 0.45535055350553505\n",
            "b_accuracy french val 0.48139340501219013\n",
            "None\n",
            "accuracy french train 0.4603174603174603\n",
            "b_accuracy french train 0.4580553323464065\n",
            "None\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 52ms/step - loss: 3.2561 - accuracy: 0.4120 - val_loss: 2.4347 - val_accuracy: 0.4945\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.7240 - accuracy: 0.4720 - val_loss: 2.1760 - val_accuracy: 0.5048\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 2.3832 - accuracy: 0.4720 - val_loss: 2.0528 - val_accuracy: 0.5159\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 2.3072 - accuracy: 0.5000 - val_loss: 1.9838 - val_accuracy: 0.5336\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.9497 - accuracy: 0.4800 - val_loss: 1.9625 - val_accuracy: 0.5387\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.7805 - accuracy: 0.5080 - val_loss: 1.9462 - val_accuracy: 0.5380\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.7459 - accuracy: 0.5360 - val_loss: 1.9217 - val_accuracy: 0.5461\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4552 - accuracy: 0.5240 - val_loss: 1.9020 - val_accuracy: 0.5535\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4646 - accuracy: 0.5600 - val_loss: 1.8802 - val_accuracy: 0.5631\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.4446 - accuracy: 0.5720 - val_loss: 1.8585 - val_accuracy: 0.5727\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.4175 - accuracy: 0.5560 - val_loss: 1.8384 - val_accuracy: 0.5830\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 1.2339 - accuracy: 0.5840 - val_loss: 1.8353 - val_accuracy: 0.5793\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.2886 - accuracy: 0.5920 - val_loss: 1.8417 - val_accuracy: 0.5823\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.1928 - accuracy: 0.6040 - val_loss: 1.8457 - val_accuracy: 0.5948\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.9706 - accuracy: 0.6440 - val_loss: 1.8510 - val_accuracy: 0.5941\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.0192 - accuracy: 0.6480 - val_loss: 1.8501 - val_accuracy: 0.6015\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.0102 - accuracy: 0.6160 - val_loss: 1.8313 - val_accuracy: 0.6030\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.8719 - accuracy: 0.6600 - val_loss: 1.8245 - val_accuracy: 0.6118\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 1.0362 - accuracy: 0.6480 - val_loss: 1.8203 - val_accuracy: 0.6059\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.9829 - accuracy: 0.6360 - val_loss: 1.8303 - val_accuracy: 0.6096\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.8819 - accuracy: 0.6640 - val_loss: 1.8198 - val_accuracy: 0.6125\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.8028 - accuracy: 0.6880 - val_loss: 1.8164 - val_accuracy: 0.6133\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.8430 - accuracy: 0.6960 - val_loss: 1.8138 - val_accuracy: 0.6162\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.7810 - accuracy: 0.7440 - val_loss: 1.8197 - val_accuracy: 0.6133\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.7263 - accuracy: 0.7200 - val_loss: 1.8092 - val_accuracy: 0.6199\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.7676 - accuracy: 0.6880 - val_loss: 1.7977 - val_accuracy: 0.6229\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.7581 - accuracy: 0.7080 - val_loss: 1.7967 - val_accuracy: 0.6214\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.6486 - accuracy: 0.7520 - val_loss: 1.8036 - val_accuracy: 0.6251\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.7289 - accuracy: 0.7200 - val_loss: 1.7952 - val_accuracy: 0.6317\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.7253 - accuracy: 0.7200 - val_loss: 1.7893 - val_accuracy: 0.6280\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.6239 - accuracy: 0.7600 - val_loss: 1.7789 - val_accuracy: 0.6288\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.6352 - accuracy: 0.7400 - val_loss: 1.7820 - val_accuracy: 0.6280\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.5515 - accuracy: 0.7640 - val_loss: 1.7866 - val_accuracy: 0.6310\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.5728 - accuracy: 0.7600 - val_loss: 1.7925 - val_accuracy: 0.6288\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.5318 - accuracy: 0.8160 - val_loss: 1.7959 - val_accuracy: 0.6251\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.6576 - accuracy: 0.7760 - val_loss: 1.8171 - val_accuracy: 0.6221\n",
            "250\n",
            "accuracy german 0.8397389412617839\n",
            "b_accuracy german 0.8436633197867811\n",
            "None\n",
            "accuracy french test 0.6637168141592921\n",
            "b_accuracy french test 0.6463302419567574\n",
            "None\n",
            "accuracy french val 0.622140221402214\n",
            "b_accuracy french val 0.5855403089345761\n",
            "None\n",
            "accuracy french train 0.6517165005537099\n",
            "b_accuracy french train 0.6095754020756385\n",
            "None\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 3.6152 - accuracy: 0.6300 - val_loss: 1.6407 - val_accuracy: 0.6310\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 2.4796 - accuracy: 0.6260 - val_loss: 1.5347 - val_accuracy: 0.6339\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.9362 - accuracy: 0.6600 - val_loss: 1.4856 - val_accuracy: 0.6332\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.5987 - accuracy: 0.6520 - val_loss: 1.4642 - val_accuracy: 0.6332\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.7439 - accuracy: 0.6120 - val_loss: 1.4463 - val_accuracy: 0.6354\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.4164 - accuracy: 0.6540 - val_loss: 1.3882 - val_accuracy: 0.6517\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.2049 - accuracy: 0.6640 - val_loss: 1.3576 - val_accuracy: 0.6561\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.3003 - accuracy: 0.6940 - val_loss: 1.3436 - val_accuracy: 0.6642\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 1.1344 - accuracy: 0.6500 - val_loss: 1.3303 - val_accuracy: 0.6657\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 1.1202 - accuracy: 0.6900 - val_loss: 1.3042 - val_accuracy: 0.6686\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.9189 - accuracy: 0.7080 - val_loss: 1.2955 - val_accuracy: 0.6701\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.8378 - accuracy: 0.7540 - val_loss: 1.2980 - val_accuracy: 0.6708\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.8669 - accuracy: 0.7400 - val_loss: 1.2888 - val_accuracy: 0.6768\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.8813 - accuracy: 0.7060 - val_loss: 1.2797 - val_accuracy: 0.6701\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.9934 - accuracy: 0.7100 - val_loss: 1.2719 - val_accuracy: 0.6723\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.7354 - accuracy: 0.7520 - val_loss: 1.2647 - val_accuracy: 0.6731\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.7586 - accuracy: 0.7420 - val_loss: 1.2520 - val_accuracy: 0.6738\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.7064 - accuracy: 0.7760 - val_loss: 1.2533 - val_accuracy: 0.6782\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.7498 - accuracy: 0.7480 - val_loss: 1.2426 - val_accuracy: 0.6827\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.7335 - accuracy: 0.7360 - val_loss: 1.2379 - val_accuracy: 0.6841\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6551 - accuracy: 0.7840 - val_loss: 1.2386 - val_accuracy: 0.6841\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6326 - accuracy: 0.7720 - val_loss: 1.2277 - val_accuracy: 0.6900\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6248 - accuracy: 0.7740 - val_loss: 1.2257 - val_accuracy: 0.6871\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6305 - accuracy: 0.7840 - val_loss: 1.2271 - val_accuracy: 0.6863\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5209 - accuracy: 0.7960 - val_loss: 1.2357 - val_accuracy: 0.6804\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.6024 - accuracy: 0.7560 - val_loss: 1.2263 - val_accuracy: 0.6827\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5529 - accuracy: 0.7800 - val_loss: 1.2240 - val_accuracy: 0.6915\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.4969 - accuracy: 0.7860 - val_loss: 1.2245 - val_accuracy: 0.6863\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5513 - accuracy: 0.7880 - val_loss: 1.2254 - val_accuracy: 0.6923\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5269 - accuracy: 0.8140 - val_loss: 1.2195 - val_accuracy: 0.6923\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.4983 - accuracy: 0.7980 - val_loss: 1.2148 - val_accuracy: 0.6996\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5251 - accuracy: 0.7940 - val_loss: 1.2261 - val_accuracy: 0.6989\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5229 - accuracy: 0.7920 - val_loss: 1.2320 - val_accuracy: 0.6996\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.5285 - accuracy: 0.7960 - val_loss: 1.2211 - val_accuracy: 0.6982\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.4994 - accuracy: 0.8140 - val_loss: 1.2167 - val_accuracy: 0.7018\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.4988 - accuracy: 0.8100 - val_loss: 1.2192 - val_accuracy: 0.6996\n",
            "500\n",
            "accuracy german 0.8578680203045685\n",
            "b_accuracy german 0.8206905507233656\n",
            "None\n",
            "accuracy french test 0.7522123893805309\n",
            "b_accuracy french test 0.6824730555603401\n",
            "None\n",
            "accuracy french val 0.6996309963099631\n",
            "b_accuracy french val 0.6728619462154838\n",
            "None\n",
            "accuracy french train 0.7388335179032853\n",
            "b_accuracy french train 0.7079599241341517\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 1.2629 - accuracy: 0.7380 - val_loss: 1.1522 - val_accuracy: 0.7041\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.0767 - accuracy: 0.7340 - val_loss: 1.1271 - val_accuracy: 0.7063\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9326 - accuracy: 0.7300 - val_loss: 1.1265 - val_accuracy: 0.7026\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8450 - accuracy: 0.7590 - val_loss: 1.1201 - val_accuracy: 0.7004\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.8057 - accuracy: 0.7400 - val_loss: 1.1038 - val_accuracy: 0.7122\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7855 - accuracy: 0.7650 - val_loss: 1.0972 - val_accuracy: 0.7159\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6906 - accuracy: 0.7810 - val_loss: 1.1012 - val_accuracy: 0.7107\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5837 - accuracy: 0.7840 - val_loss: 1.0873 - val_accuracy: 0.7181\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6843 - accuracy: 0.7750 - val_loss: 1.0807 - val_accuracy: 0.7232\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5683 - accuracy: 0.7930 - val_loss: 1.0855 - val_accuracy: 0.7196\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.5820 - accuracy: 0.7870 - val_loss: 1.0856 - val_accuracy: 0.7166\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5866 - accuracy: 0.7860 - val_loss: 1.0806 - val_accuracy: 0.7247\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5538 - accuracy: 0.8050 - val_loss: 1.0664 - val_accuracy: 0.7314\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5150 - accuracy: 0.7990 - val_loss: 1.0629 - val_accuracy: 0.7380\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.5072 - accuracy: 0.7940 - val_loss: 1.0764 - val_accuracy: 0.7284\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4822 - accuracy: 0.8150 - val_loss: 1.0794 - val_accuracy: 0.7343\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4994 - accuracy: 0.8130 - val_loss: 1.0635 - val_accuracy: 0.7432\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4586 - accuracy: 0.8110 - val_loss: 1.0716 - val_accuracy: 0.7410\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4614 - accuracy: 0.8180 - val_loss: 1.0709 - val_accuracy: 0.7461\n",
            "1000\n",
            "accuracy german 0.8716461203770849\n",
            "b_accuracy german 0.8286833737328293\n",
            "None\n",
            "accuracy french test 0.7986725663716814\n",
            "b_accuracy french test 0.759541078526041\n",
            "None\n",
            "accuracy french val 0.7461254612546125\n",
            "b_accuracy french val 0.6917658365406675\n",
            "None\n",
            "accuracy french train 0.8049095607235142\n",
            "b_accuracy french train 0.7614082295473696\n",
            "None\n",
            "Epoch 1/100\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 1.0487 - accuracy: 0.7445 - val_loss: 1.0023 - val_accuracy: 0.7328\n",
            "Epoch 2/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.8697 - accuracy: 0.7515 - val_loss: 0.9772 - val_accuracy: 0.7439\n",
            "Epoch 3/100\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.8596 - accuracy: 0.7555 - val_loss: 0.9559 - val_accuracy: 0.7454\n",
            "Epoch 4/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.8167 - accuracy: 0.7495 - val_loss: 0.9381 - val_accuracy: 0.7483\n",
            "Epoch 5/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.7269 - accuracy: 0.7675 - val_loss: 0.9248 - val_accuracy: 0.7528\n",
            "Epoch 6/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.6898 - accuracy: 0.7740 - val_loss: 0.9081 - val_accuracy: 0.7557\n",
            "Epoch 7/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.6569 - accuracy: 0.7800 - val_loss: 0.9021 - val_accuracy: 0.7579\n",
            "Epoch 8/100\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.6737 - accuracy: 0.7855 - val_loss: 0.8982 - val_accuracy: 0.7565\n",
            "Epoch 9/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.6549 - accuracy: 0.7735 - val_loss: 0.8980 - val_accuracy: 0.7579\n",
            "Epoch 10/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.6161 - accuracy: 0.7780 - val_loss: 0.8902 - val_accuracy: 0.7601\n",
            "Epoch 11/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.5796 - accuracy: 0.7840 - val_loss: 0.8868 - val_accuracy: 0.7594\n",
            "Epoch 12/100\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.5087 - accuracy: 0.8085 - val_loss: 0.8735 - val_accuracy: 0.7712\n",
            "Epoch 13/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.5314 - accuracy: 0.7940 - val_loss: 0.8713 - val_accuracy: 0.7734\n",
            "Epoch 14/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.5250 - accuracy: 0.8040 - val_loss: 0.8722 - val_accuracy: 0.7638\n",
            "Epoch 15/100\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.4826 - accuracy: 0.8060 - val_loss: 0.8733 - val_accuracy: 0.7756\n",
            "Epoch 16/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4585 - accuracy: 0.8245 - val_loss: 0.8734 - val_accuracy: 0.7697\n",
            "Epoch 17/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4664 - accuracy: 0.8205 - val_loss: 0.8575 - val_accuracy: 0.7720\n",
            "Epoch 18/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4860 - accuracy: 0.8195 - val_loss: 0.8657 - val_accuracy: 0.7845\n",
            "Epoch 19/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4639 - accuracy: 0.8205 - val_loss: 0.8689 - val_accuracy: 0.7823\n",
            "Epoch 20/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4636 - accuracy: 0.8250 - val_loss: 0.8645 - val_accuracy: 0.7801\n",
            "Epoch 21/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4353 - accuracy: 0.8215 - val_loss: 0.8597 - val_accuracy: 0.7852\n",
            "Epoch 22/100\n",
            "63/63 [==============================] - 1s 9ms/step - loss: 0.4025 - accuracy: 0.8360 - val_loss: 0.8600 - val_accuracy: 0.7801\n",
            "2000\n",
            "accuracy german 0.854967367657723\n",
            "b_accuracy german 0.8277556266902171\n",
            "None\n",
            "accuracy french test 0.8163716814159292\n",
            "b_accuracy french test 0.8001940004911142\n",
            "None\n",
            "accuracy french val 0.7800738007380074\n",
            "b_accuracy french val 0.7430519124381357\n",
            "None\n",
            "accuracy french train 0.8584348468069398\n",
            "b_accuracy french train 0.8350253160401225\n",
            "None\n",
            "Epoch 1/100\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 0.9301 - accuracy: 0.7678 - val_loss: 0.8084 - val_accuracy: 0.7845\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.8334 - accuracy: 0.7736 - val_loss: 0.7728 - val_accuracy: 0.7926\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.7081 - accuracy: 0.7880 - val_loss: 0.7365 - val_accuracy: 0.8037\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.6872 - accuracy: 0.7844 - val_loss: 0.7126 - val_accuracy: 0.8125\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.6265 - accuracy: 0.8000 - val_loss: 0.6967 - val_accuracy: 0.8081\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.6068 - accuracy: 0.8004 - val_loss: 0.6869 - val_accuracy: 0.8148\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.5935 - accuracy: 0.8054 - val_loss: 0.6482 - val_accuracy: 0.8303\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.5566 - accuracy: 0.8038 - val_loss: 0.6503 - val_accuracy: 0.8199\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.5281 - accuracy: 0.8160 - val_loss: 0.6276 - val_accuracy: 0.8244\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4984 - accuracy: 0.8168 - val_loss: 0.6215 - val_accuracy: 0.8288\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.5211 - accuracy: 0.8168 - val_loss: 0.6236 - val_accuracy: 0.8317\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4819 - accuracy: 0.8148 - val_loss: 0.5927 - val_accuracy: 0.8450\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4590 - accuracy: 0.8320 - val_loss: 0.5810 - val_accuracy: 0.8487\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4487 - accuracy: 0.8314 - val_loss: 0.5809 - val_accuracy: 0.8443\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4332 - accuracy: 0.8388 - val_loss: 0.5709 - val_accuracy: 0.8435\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4104 - accuracy: 0.8466 - val_loss: 0.5537 - val_accuracy: 0.8509\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.4098 - accuracy: 0.8404 - val_loss: 0.5578 - val_accuracy: 0.8472\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3825 - accuracy: 0.8494 - val_loss: 0.5498 - val_accuracy: 0.8517\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3889 - accuracy: 0.8540 - val_loss: 0.5388 - val_accuracy: 0.8539\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3594 - accuracy: 0.8498 - val_loss: 0.5287 - val_accuracy: 0.8605\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3583 - accuracy: 0.8584 - val_loss: 0.5285 - val_accuracy: 0.8598\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3405 - accuracy: 0.8636 - val_loss: 0.5173 - val_accuracy: 0.8686\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3270 - accuracy: 0.8672 - val_loss: 0.5086 - val_accuracy: 0.8686\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3425 - accuracy: 0.8658 - val_loss: 0.5085 - val_accuracy: 0.8627\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3351 - accuracy: 0.8656 - val_loss: 0.5018 - val_accuracy: 0.8649\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3023 - accuracy: 0.8768 - val_loss: 0.4954 - val_accuracy: 0.8694\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3190 - accuracy: 0.8752 - val_loss: 0.4997 - val_accuracy: 0.8686\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3073 - accuracy: 0.8734 - val_loss: 0.4759 - val_accuracy: 0.8782\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3150 - accuracy: 0.8746 - val_loss: 0.4840 - val_accuracy: 0.8753\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3058 - accuracy: 0.8704 - val_loss: 0.4841 - val_accuracy: 0.8775\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.3064 - accuracy: 0.8742 - val_loss: 0.4653 - val_accuracy: 0.8812\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2943 - accuracy: 0.8760 - val_loss: 0.4741 - val_accuracy: 0.8775\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2868 - accuracy: 0.8794 - val_loss: 0.4610 - val_accuracy: 0.8797\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2769 - accuracy: 0.8806 - val_loss: 0.4586 - val_accuracy: 0.8782\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2549 - accuracy: 0.8872 - val_loss: 0.4520 - val_accuracy: 0.8878\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2788 - accuracy: 0.8858 - val_loss: 0.4489 - val_accuracy: 0.8804\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2740 - accuracy: 0.8840 - val_loss: 0.4359 - val_accuracy: 0.8900\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2488 - accuracy: 0.8948 - val_loss: 0.4427 - val_accuracy: 0.8886\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2510 - accuracy: 0.8916 - val_loss: 0.4360 - val_accuracy: 0.8915\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2543 - accuracy: 0.8894 - val_loss: 0.4391 - val_accuracy: 0.8871\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2394 - accuracy: 0.8956 - val_loss: 0.4346 - val_accuracy: 0.8893\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2505 - accuracy: 0.8952 - val_loss: 0.4428 - val_accuracy: 0.8900\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2389 - accuracy: 0.8942 - val_loss: 0.4261 - val_accuracy: 0.8915\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2398 - accuracy: 0.8980 - val_loss: 0.4303 - val_accuracy: 0.8908\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2309 - accuracy: 0.8986 - val_loss: 0.4328 - val_accuracy: 0.8878\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2270 - accuracy: 0.9004 - val_loss: 0.4284 - val_accuracy: 0.8900\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2218 - accuracy: 0.9040 - val_loss: 0.4264 - val_accuracy: 0.8900\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.2180 - accuracy: 0.9040 - val_loss: 0.4174 - val_accuracy: 0.8952\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2052 - accuracy: 0.9054 - val_loss: 0.4145 - val_accuracy: 0.8945\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2152 - accuracy: 0.9072 - val_loss: 0.4225 - val_accuracy: 0.8937\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2014 - accuracy: 0.9072 - val_loss: 0.4195 - val_accuracy: 0.9004\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2010 - accuracy: 0.9038 - val_loss: 0.4174 - val_accuracy: 0.8967\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.2032 - accuracy: 0.9152 - val_loss: 0.4148 - val_accuracy: 0.8974\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1980 - accuracy: 0.9082 - val_loss: 0.4117 - val_accuracy: 0.8989\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1870 - accuracy: 0.9146 - val_loss: 0.4083 - val_accuracy: 0.8989\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1922 - accuracy: 0.9132 - val_loss: 0.4069 - val_accuracy: 0.9011\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1919 - accuracy: 0.9140 - val_loss: 0.4089 - val_accuracy: 0.8982\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1769 - accuracy: 0.9170 - val_loss: 0.4083 - val_accuracy: 0.9004\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1876 - accuracy: 0.9162 - val_loss: 0.4036 - val_accuracy: 0.9018\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1795 - accuracy: 0.9162 - val_loss: 0.3962 - val_accuracy: 0.9004\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1906 - accuracy: 0.9138 - val_loss: 0.4020 - val_accuracy: 0.8989\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1711 - accuracy: 0.9206 - val_loss: 0.4018 - val_accuracy: 0.9011\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1776 - accuracy: 0.9170 - val_loss: 0.4075 - val_accuracy: 0.9041\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1724 - accuracy: 0.9204 - val_loss: 0.3964 - val_accuracy: 0.9063\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1702 - accuracy: 0.9188 - val_loss: 0.3920 - val_accuracy: 0.9100\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1635 - accuracy: 0.9254 - val_loss: 0.3947 - val_accuracy: 0.9026\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1632 - accuracy: 0.9230 - val_loss: 0.3907 - val_accuracy: 0.9077\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1713 - accuracy: 0.9206 - val_loss: 0.3892 - val_accuracy: 0.9107\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1660 - accuracy: 0.9244 - val_loss: 0.3997 - val_accuracy: 0.9048\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1673 - accuracy: 0.9214 - val_loss: 0.3865 - val_accuracy: 0.9041\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 0.1533 - accuracy: 0.9278 - val_loss: 0.3955 - val_accuracy: 0.9100\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1659 - accuracy: 0.9234 - val_loss: 0.3871 - val_accuracy: 0.9129\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1472 - accuracy: 0.9292 - val_loss: 0.3956 - val_accuracy: 0.9107\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1585 - accuracy: 0.9222 - val_loss: 0.3897 - val_accuracy: 0.9151\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.1528 - accuracy: 0.9266 - val_loss: 0.3959 - val_accuracy: 0.9092\n",
            "5000\n",
            "accuracy german 0.7171863669325598\n",
            "b_accuracy german 0.7613965832137914\n",
            "None\n",
            "accuracy french test 0.918141592920354\n",
            "b_accuracy french test 0.9177292833238802\n",
            "None\n",
            "accuracy french val 0.9092250922509225\n",
            "b_accuracy french val 0.9224471351314723\n",
            "None\n",
            "accuracy french train 0.9776670358065707\n",
            "b_accuracy french train 0.9817927641527955\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwiSgh6SjS1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_xy(X_test_emb_de,y_test_de,'german',de_cnn2d_transfer)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',de_cnn2d_transfer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-alnrBeUhh9M",
        "colab_type": "text"
      },
      "source": [
        "2D trial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0aMETWFnrKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class text_to_embed(object):\n",
        "    '''\n",
        "    takes text and embeddingmodel as input and outputs sequence of embeddings\n",
        "    '''\n",
        "    def __init__(self\n",
        "                 , text = None\n",
        "                 , embed_de = None\n",
        "                 , embed_fr = None\n",
        "                 , seq_len = None\n",
        "                 , rep_dict = rep_dict\n",
        "                 , embedding_dim=300):\n",
        "                 \n",
        "        self.text = text\n",
        "        self.embed_de = embed_de\n",
        "        self.embed_fr = embed_fr\n",
        "        self.seq_len = seq_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def t2s(self,line):\n",
        "        sen_embed = np.zeros((self.embedding_dim,self.seq_len))\n",
        "        words = line.split()\n",
        "        for w in range(0,self.seq_len):\n",
        "            try: \n",
        "                emb_fr = self.embed_fr[words[w]]\n",
        "                emb_de = self.embed_de[words[w]]\n",
        "                \n",
        "            except:\n",
        "                emb_de = np.zeros(embedding_dim)\n",
        "                emb_fr = np.zeros(embedding_dim)\n",
        "            #try: \n",
        "            #    emb_fr = self.embed_fr[words[w]]\n",
        "            #except:\n",
        "            #    emb_fr = np.zeros(embedding_dim)\n",
        "\n",
        "            emb_de = np.expand_dims(emb_de, axis=0)\n",
        "            emb_fr = np.expand_dims(emb_fr, axis=0)\n",
        "\n",
        "            if w == 0 :\n",
        "                sen_embed_de = emb_de\n",
        "                sen_embed_fr = emb_fr\n",
        "            else:\n",
        "                sen_embed_de = np.concatenate([sen_embed_de,emb_de],axis=0)\n",
        "                sen_embed_fr = np.concatenate([sen_embed_fr,emb_fr],axis=0)\n",
        "\n",
        "        sen_emb = np.stack([sen_embed_de,sen_embed_fr],axis=0)\n",
        "        return sen_emb #np.array(tokens)\n",
        "\n",
        "    def t2s_old(self,line):\n",
        "        #tokens = []\n",
        "        sen_embed = np.zeros((self.embedding_dim,self.seq_len))\n",
        "        words = line.split()\n",
        "        for w in range(0,self.seq_len):\n",
        "            try: \n",
        "              if la == 'de':\n",
        "                  emb = self.embed_de[words[w]]\n",
        "              elif la == 'fr':\n",
        "                  emb = self.embed_fr[words[w]]\n",
        "              else:\n",
        "                  print(la)\n",
        "                  emb = np.zeros(self.embedding_dim)\n",
        "            except:\n",
        "                emb = np.zeros(self.embedding_dim)\n",
        "            #tokens.append(tok)\n",
        "            sen_embed[:,w] = emb\n",
        "\n",
        "        sen_embed = np.swapaxes(sen_embed,0,1)\n",
        "        return sen_embed #np.array(tokens)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in tqdm(self.text):\n",
        "            line = self.t2s(line)\n",
        "            yield line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK7NG-6DovR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_emb_de = np.array(list(text_to_embed(X_train_de,  de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_val_emb_de = np.array(list(text_to_embed(X_val_de, de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_test_emb_de = np.array(list(text_to_embed(X_test_de, de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "\n",
        "X_train_emb_fr = np.array(list(text_to_embed(X_train_fr, de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_val_emb_fr = np.array(list(text_to_embed(X_val_fr, de_git_embed, fr_git_embed, seq_len=seq_len)))\n",
        "X_test_emb_fr = np.array(list(text_to_embed(X_test_fr, de_git_embed, fr_git_embed, seq_len=seq_len)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9l9t7lkXE_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('de',X_train_emb_de.shape, X_val_emb_de.shape, X_test_emb_de.shape)\n",
        "print('fr',X_train_emb_fr.shape, X_val_emb_fr.shape, X_test_emb_fr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqj8BjLldKm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_emb_de = np.swapaxes(X_train_emb_de,2,3)\n",
        "X_val_emb_de = np.swapaxes(X_val_emb_de,2,3)\n",
        "X_test_emb_de = np.swapaxes(X_test_emb_de,2,3)\n",
        "\n",
        "X_train_emb_fr = np.swapaxes(X_train_emb_fr,2,3)\n",
        "X_val_emb_fr = np.swapaxes(X_val_emb_fr,2,3)\n",
        "X_test_emb_fr = np.swapaxes(X_test_emb_fr,2,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t61BbGrZXEvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,GlobalAveragePooling2D,Dropout,Flatten,AveragePooling2D\n",
        "dropout_rate=.30\n",
        "\n",
        "input_layer = Input(shape = (2,embedding_dim,seq_len,), name='text_input')\n",
        "#pool_layer = AveragePooling2D(pool_size=(2, 1),name='avg_pooling2')(input_layer)\n",
        "conv_layer = Conv2D(10, kernel_size=(100, 10), activation='relu', padding=\"same\")(input_layer)\n",
        "pool_layer = AveragePooling2D(pool_size=(1, 5),name='avg_pooling3')(conv_layer)\n",
        "pool_layer = AveragePooling2D(pool_size=(2, 1),name='avg_pooling2')(pool_layer)\n",
        "conv_layer = Conv2D(5, kernel_size=(50, 2), activation='relu', padding=\"same\")(pool_layer)\n",
        "#pool_layer = AveragePooling2D(pool_size=(1, 5),name='avg_pooling3')(conv_layer)\n",
        "#conv_layer = Conv2D(50, kernel_size=(3, 3), activation='relu', padding=\"same\")(conv_layer)\n",
        "#conv_layer = AveragePooling2D(pool_size=(1, 3),name='avg_pooling3')(conv_layer)\n",
        "#mask_layer = Masking(mask_value=0.)(input_layer)\n",
        "#pool_layer = AveragePooling2D(pool_size=(2, 1),name='avg_pooling')(conv_layer)\n",
        "pool_layer = Flatten()(conv_layer)\n",
        "#pool_layer = GlobalAveragePooling2D(name='avg_pooling')(pool_layer)\n",
        "dens_layer = Dense(150, activation='relu',name='dense')(pool_layer)\n",
        "drop_layer = Dropout(dropout_rate)(pool_layer)\n",
        "pred_layer_3 = Dense(no_Classes, activation = 'softmax', name='out')(drop_layer) \n",
        "\n",
        "de_avg_pool = Model(inputs = [input_layer], outputs = [pred_layer_3])\n",
        "\n",
        "lr = .001\n",
        "opt = Adam(lr=lr, decay=lr/200)\n",
        "de_avg_pool.summary()\n",
        "de_avg_pool.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "hist = de_avg_pool.fit(x = X_train_emb_de, y = y_train_enc_de,\\\n",
        "                validation_data = (X_val_emb_de, y_val_enc_de), \\\n",
        "                epochs = 100, batch_size = 256, shuffle = True, \\\n",
        "                class_weight = class_weight_dict_de, \\\n",
        "                callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV1nqSl1XEr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,balanced_accuracy_score,confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def test_xy(X,y,string,model):\n",
        "    y_pred = model.predict([X])\n",
        "    y_pred_arg = y_pred.argmax(axis=1)\n",
        "    pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "    print('accuracy %s'% string,accuracy_score(pred,y))\n",
        "    print('b_accuracy %s'%  string,balanced_accuracy_score(pred, y))\n",
        "    #return pred, y\n",
        "\n",
        "#test_xy(X_test_emb_de,y_test_de,'together')\n",
        "test_xy(X_test_emb_de,y_test_de,'german',de_avg_pool)\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french',de_avg_pool)\n",
        "#print(classification_report(y,pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yho3MuOCQkNE",
        "colab_type": "text"
      },
      "source": [
        "aa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zESsGd7UHiw",
        "colab_type": "text"
      },
      "source": [
        "# Multilingual Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmpt9P4RQxqn",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "150UxiyYfHBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_emb =  np.concatenate((X_train_emb_de, X_train_emb_fr))\n",
        "y_train_enc =  np.concatenate((y_train_enc_de, y_train_enc_fr))\n",
        "X_val_emb =  np.concatenate((X_val_emb_de, X_val_emb_fr))\n",
        "y_val_enc =  np.concatenate((y_val_enc_de, y_val_enc_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVr10pC0E_Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, Concatenate,LSTM,BatchNormalization,Add,Masking,GlobalMaxPooling1D,Conv1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "dropout_rate=.7\n",
        "\n",
        "input_imbd_1 = Input(shape = (seq_len,embedding_dim,), name='text_input')\n",
        "mask_layer = Masking(mask_value=0.)(input_imbd_1)\n",
        "conv_layer = Conv1D(filters=300,   kernel_size= 10,   padding='valid',  activation='relu', strides=1)(input_imbd_1)\n",
        "conv_layer2 = Conv1D(filters=100,   kernel_size= 5,   padding='valid',  activation='relu', strides=2)(input_imbd_1)\n",
        "#lstm_layer_1 = Bidirectional(LSTM(128,activation = 'tanh',recurrent_activation = 'sigmoid', dropout = dropout_rate,recurrent_dropout = 0, unroll = False,use_bias = True))(conv_layer)\n",
        "lstm_layer_1 = GlobalAveragePooling1D()(conv_layer)\n",
        "#lstm_layer_1 = GlobalMaxPooling1D()(conv_layer)\n",
        "dens_layer_1 = Dense(100, activation='relu')(lstm_layer_1)\n",
        "drop_layer_1 = Dropout(dropout_rate)(dens_layer_1)\n",
        "pred_layer_3 = Dense(74, activation = 'softmax', name='o3')(drop_layer_1) \n",
        "\n",
        "de_cnn_avg = Model(inputs = [input_imbd_1], outputs = [pred_layer_3])\n",
        "\n",
        "lr = .001\n",
        "opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "de_cnn_avg.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "de_cnn_avg.summary()\n",
        "hist = de_cnn_avg.fit(x = X_train_emb, y = y_train_enc,\\\n",
        "                validation_data = (X_val_emb, y_val_enc), \\\n",
        "                epochs = 150, batch_size = 64, shuffle = True, \\\n",
        "                class_weight = class_weight_dict_de_fr, \\\n",
        "                callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RcuXJqptjaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def test_xy(X,y,string):\n",
        "    y_pred = de_cnn_avg.predict([X])\n",
        "    y_pred_arg = y_pred.argmax(axis=1)\n",
        "    pred= [encoder.classes_[y] for y in y_pred_arg]\n",
        "    print('accuracy %s'% string,accuracy_score(pred,y))\n",
        "    print('b_accuracy %s'%  string,balanced_accuracy_score(pred, y))\n",
        "\n",
        "test_xy(X_test_emb_de,y_test_de,'german')\n",
        "test_xy(X_test_emb_fr,y_test_fr,'french')\n",
        "#print(classification_report(pred, df_test['cc5']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYOFTWRwvRq",
        "colab_type": "text"
      },
      "source": [
        "## CNN with training embedding ayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4dwTf0H7JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        " def run_ml_CNN(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "   \n",
        "    X_train_pad =  np.concatenate((X_train_pad_de, X_train_pad_fr[:no]))\n",
        "    y_train_enc =  np.concatenate((y_train_enc_de, y_train_enc_fr[:no]))\n",
        "\n",
        "    imbala = int(len(X_train_pad_de)/no)\n",
        "    for n in range(1,imbala):\n",
        "        X_train_pad =  np.concatenate((X_train_pad, X_train_pad_fr[:no]))\n",
        "        y_train_enc =  np.concatenate((y_train_enc, y_train_enc_fr[:no]))\n",
        "    X_val_pad =  np.concatenate((X_val_pad_de, X_val_pad_fr))\n",
        "    y_val_enc =  np.concatenate((y_val_enc_de, y_val_enc_fr))\n",
        "    print(len(X_train_pad))\n",
        "\n",
        "    dropout_rate=.7\n",
        "    lr = .0003\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), dtype = 'float')\n",
        "    embedd_seq = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = seq_len, trainable = True, mask_zero=False)(input_layer)\n",
        "    norma_layer = BatchNormalization()(embedd_seq)\n",
        "    convo_layer = Conv1D(filters=300,   kernel_size= 5,   padding='valid',  activation='relu', strides=1)(norma_layer)\n",
        "    norma_layer = BatchNormalization()(convo_layer)\n",
        "    pool_layer = GlobalMaxPooling1D()(norma_layer)\n",
        "    #pool_layer = GlobalAveragePooling1D()(convo_layer)\n",
        "    dens_layer_1 = Dense(150, activation='relu')(pool_layer)\n",
        "    drop_layer = Dropout(dropout_rate)(dens_layer_1)\n",
        "    pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "    cnn_de_fr = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "    cnn_de_fr.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = cnn_de_fr.fit(x = X_train_pad, y = y_train_enc,\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test_de = cnn_de_fr.predict(X_test_pad_de)\n",
        "    y_pred_arg_de = y_pred_test_de.argmax(axis=1)\n",
        "    y_pred_test_de = [encoder.classes_[y] for y in y_pred_arg_de]\n",
        "\n",
        "    y_pred_test_fr = cnn_de_fr.predict(X_test_pad_fr)\n",
        "    y_pred_arg_fr = y_pred_test_fr.argmax(axis=1)\n",
        "    y_pred_test_fr= [encoder.classes_[y] for y in y_pred_arg_fr]\n",
        "\n",
        "    print('de test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('de test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('fr test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_fr, y_test_fr))\n",
        "    print('fr test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_fr, y_test_fr))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3AiEmrgJrE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for obs in [500,1000,2000,5000]:\n",
        "     run_ml_CNN(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKXvkCXbIxP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_CNN_best(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0m56K1rHIv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "dropout_rate=.6\n",
        "lr = .001\n",
        "opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "input_layer = Input(shape = (seq_len,), dtype = 'float')\n",
        "embedd_seq = Embedding(vocab_size_de_fr, embedding_dim, weights = [embedding_matrix_de_fr], input_length = seq_len, trainable = True, mask_zero=False)(input_layer)\n",
        "conv_layer = Conv1D(filters=100,   kernel_size= 5,   padding='valid',  activation='relu', strides=1)(embedd_seq)\n",
        "pool_layer = GlobalMaxPooling1D(name='max_pool')(conv_layer)\n",
        "#pool_layer = GlobalAveragePooling1D(name='avg_pooling')(conv_layer)\n",
        "dens_layer = Dense(150, activation='tanh',name='dense')(pool_layer)\n",
        "drop_layer = Dropout(dropout_rate)(dens_layer)\n",
        "pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "cnn_max_pool_mod = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "cnn_max_pool_mod.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "#print(Ftext_pool.summary())\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "hist = cnn_max_pool_mod.fit(x = X_train_pad, y = y_train_enc,\\\n",
        "                validation_data = (X_val_pad, y_val_enc), \\\n",
        "                epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict_de_fr, \\\n",
        "                callbacks = [early_stopping])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "y_pred_t_de = cnn_max_pool_mod.predict(X_test_pad_de)\n",
        "y_pred_arg = y_pred_t_de.argmax(axis=1)\n",
        "y_pred_t_de= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "y_pred_t_fr = cnn_max_pool_mod.predict(X_test_pad_fr)\n",
        "y_pred_arg = y_pred_t_fr.argmax(axis=1)\n",
        "y_pred_t_fr= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "print('train obs.','accuracy %s' % accuracy_score(y_pred_t_de, y_test_de))\n",
        "print('train obs.','b_accuracy %s' % balanced_accuracy_score(y_pred_t_de, y_test_de))\n",
        "print('train obs.','accuracy %s' % accuracy_score(y_pred_t_fr, y_test_fr))\n",
        "print('train obs.','b_accuracy %s' % balanced_accuracy_score(y_pred_t_fr, y_test_fr))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACPf0XjuYX4",
        "colab_type": "text"
      },
      "source": [
        "## CNN2D German - French"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ3dKOUgubCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def run_ml_CNN2D(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "   \n",
        "    X_train_pad =  np.concatenate((X_train_pad_de, X_train_pad_fr[:no]))\n",
        "    y_train_enc =  np.concatenate((y_train_enc_de, y_train_enc_fr[:no]))\n",
        "\n",
        "    imbala = int(len(X_train_pad_de)/no)\n",
        "    for n in range(1,imbala):\n",
        "        X_train_pad =  np.concatenate((X_train_pad, X_train_pad_fr[:no]))\n",
        "        y_train_enc =  np.concatenate((y_train_enc, y_train_enc_fr[:no]))\n",
        "    X_val_pad =  np.concatenate((X_val_pad_de, X_val_pad_fr))\n",
        "    y_val_enc =  np.concatenate((y_val_enc_de, y_val_enc_fr))\n",
        "    print(imbala,len(X_train_pad_de),len(X_train_pad))\n",
        "\n",
        "    dropout_rate=.4\n",
        "    filter_sizes = [1,2,3,4,5]\n",
        "    num_filters = 15\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), name='text_input')\n",
        "    x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable = True)(input_layer)\n",
        "    x = Reshape((seq_len, embedding_dim, 1))(x)\n",
        "\n",
        "    maxpool_pool = []\n",
        "    for i in range(len(filter_sizes)):\n",
        "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_dim),\n",
        "                                      kernel_initializer='he_normal', activation='relu')(x)\n",
        "        maxpool_pool.append(MaxPool2D(pool_size=(seq_len - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "    pool_layer = Concatenate(axis=1)(maxpool_pool)   \n",
        "\n",
        "    falt_layer = Flatten(name='flat')(pool_layer)\n",
        "    drop_layer = Dropout(dropout_rate,name='drop')(falt_layer)\n",
        "    pred_layer = Dense(no_Classes, activation = 'softmax', name='output_de')(drop_layer) \n",
        "\n",
        "    de_cnn2d_ml = Model(inputs = [input_layer], outputs = [pred_layer])\n",
        "\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "    de_cnn2d_ml.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = de_cnn2d_ml.fit(x = X_train_pad, y = y_train_enc,\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test_de = de_cnn2d_ml.predict(X_test_pad_de)\n",
        "    y_pred_arg_de = y_pred_test_de.argmax(axis=1)\n",
        "    y_pred_test_de = [encoder.classes_[y] for y in y_pred_arg_de]\n",
        "\n",
        "    y_pred_test_fr = de_cnn2d_ml.predict(X_test_pad_fr)\n",
        "    y_pred_arg_fr = y_pred_test_fr.argmax(axis=1)\n",
        "    y_pred_test_fr= [encoder.classes_[y] for y in y_pred_arg_fr]\n",
        "\n",
        "    print('de test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('de test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('fr test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_fr, y_test_fr))\n",
        "    print('fr test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_fr, y_test_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQMqvKEvANb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000]:\n",
        "     run_ml_CNN2D(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJoRUySFHfm",
        "colab_type": "text"
      },
      "source": [
        "## CNN2D French - German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkxVSQZyFMVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def run_ml_CNN2D(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size,embedding_matrix,class_weight_dict,no):\n",
        "   \n",
        "    X_train_pad =  np.concatenate((X_train_pad_de[:no], X_train_pad_fr))\n",
        "    y_train_enc =  np.concatenate((y_train_enc_de[:no], y_train_enc_fr))\n",
        "\n",
        "    imbala = int(len(X_train_pad_fr)/no)\n",
        "    if imbala > 1:\n",
        "        for n in range(1,imbala):\n",
        "            X_train_pad =  np.concatenate((X_train_pad, X_train_pad_de[:no]))\n",
        "            y_train_enc =  np.concatenate((y_train_enc, y_train_enc_de[:no]))\n",
        "    X_val_pad =  np.concatenate((X_val_pad_de, X_val_pad_fr))\n",
        "    y_val_enc =  np.concatenate((y_val_enc_de, y_val_enc_fr))\n",
        "    print(imbala,len(X_train_pad_de),len(X_train_pad))\n",
        "\n",
        "    dropout_rate=.5\n",
        "    filter_sizes = [1,2,3,4,5]\n",
        "    num_filters = 20\n",
        "\n",
        "    input_layer = Input(shape = (seq_len,), name='text_input')\n",
        "    x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable = True)(input_layer)\n",
        "    x = Reshape((seq_len, embedding_dim, 1))(x)\n",
        "\n",
        "    maxpool_pool = []\n",
        "    for i in range(len(filter_sizes)):\n",
        "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_dim),\n",
        "                                      kernel_initializer='he_normal', activation='relu')(x)\n",
        "        maxpool_pool.append(MaxPool2D(pool_size=(seq_len - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "    pool_layer = Concatenate(axis=1)(maxpool_pool)   \n",
        "\n",
        "    falt_layer = Flatten(name='flat')(pool_layer)\n",
        "    drop_layer = Dropout(dropout_rate,name='drop')(falt_layer)\n",
        "    pred_layer = Dense(no_Classes, activation = 'softmax', name='output_de')(drop_layer) \n",
        "\n",
        "    de_cnn2d_ml = Model(inputs = [input_layer], outputs = [pred_layer])\n",
        "\n",
        "    lr = .005\n",
        "    opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "    de_cnn2d_ml.compile(optimizer = opt, loss = ['categorical_crossentropy'], metrics = ['accuracy'])\n",
        "    early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "    hist = de_cnn2d_ml.fit(x = X_train_pad, y = y_train_enc,\\\n",
        "                    validation_data = (X_val_pad, y_val_enc), \\\n",
        "                    epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict, \\\n",
        "                    callbacks = [early_stopping])\n",
        "    \n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "    y_pred_test_de = de_cnn2d_ml.predict(X_test_pad_de)\n",
        "    y_pred_arg_de = y_pred_test_de.argmax(axis=1)\n",
        "    y_pred_test_de = [encoder.classes_[y] for y in y_pred_arg_de]\n",
        "\n",
        "    y_pred_test_fr = de_cnn2d_ml.predict(X_test_pad_fr)\n",
        "    y_pred_arg_fr = y_pred_test_fr.argmax(axis=1)\n",
        "    y_pred_test_fr= [encoder.classes_[y] for y in y_pred_arg_fr]\n",
        "\n",
        "    print('de test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('de test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_de, y_test_de))\n",
        "    print('fr test obs.',no,'accuracy %s' % accuracy_score(y_pred_test_fr, y_test_fr))\n",
        "    print('fr test obs.',no,'b_accuracy %s' % balanced_accuracy_score(y_pred_test_fr, y_test_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIzDN8hAFXx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for obs in [100,250,500,1000,2000,5000,10000,15000]:\n",
        "     run_ml_CNN2D(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,\n",
        "             X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,\n",
        "             vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CLnoMSPG6Aq",
        "colab_type": "text"
      },
      "source": [
        "## Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgLT_BZNHjO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_pad =  np.concatenate((X_train_pad_de, X_train_pad_fr))\n",
        "y_train_enc =  np.concatenate((y_train_enc_de, y_train_enc_fr))\n",
        "X_val_pad =  np.concatenate((X_val_pad_de, X_val_pad_fr))\n",
        "y_val_enc =  np.concatenate((y_val_enc_de, y_val_enc_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiWk-LrrG6Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_CNN_best(X_train_pad_fr,y_train_enc_fr,X_val_pad_fr,y_val_enc_fr,X_test_pad_fr,y_test_fr,vocab_size_fr,embedding_matrix_fr,class_weight_dict_fr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAXozVg99eLK",
        "colab_type": "text"
      },
      "source": [
        "## CNN Zero Shot with training embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW59y8D69kbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_rate=.7\n",
        "lr = .001\n",
        "opt = Adam(lr=lr, decay=lr/100)\n",
        "\n",
        "input_layer = Input(shape = (seq_len,), dtype = 'float')\n",
        "embedd_seq = Embedding(vocab_size_de_fr, embedding_dim, weights = [embedding_matrix_de_fr], input_length = seq_len, trainable = True, mask_zero=False)(input_layer)\n",
        "conv_layer = Conv1D(filters=150,   kernel_size= 7,   padding='valid',  activation='relu', strides=1)(embedd_seq)\n",
        "pool_layer = GlobalMaxPooling1D(name='max_pool')(conv_layer)\n",
        "drop_layer = Dropout(dropout_rate)(pool_layer)\n",
        "pred_layer = Dense(y_train_enc.shape[1], activation = 'softmax')(drop_layer) #, kernel_regularizer=l1_l2(0.001)\n",
        "\n",
        "cnn_max_pool_mod = Model(inputs = [input_layer], outputs = pred_layer)\n",
        "cnn_max_pool_mod.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "#print(Ftext_pool.summary())\n",
        "early_stopping = EarlyStopping(patience = 5)\n",
        "\n",
        "hist = cnn_max_pool_mod.fit(x = X_train_pad_de, y = y_train_enc_de,\\\n",
        "                validation_data = (X_val_pad_de, y_val_enc_de), \\\n",
        "                epochs = 50, batch_size = 256, shuffle = True, class_weight = class_weight_dict_de_fr, \\\n",
        "                callbacks = [early_stopping])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "y_pred_t_de = cnn_max_pool_mod.predict(X_test_pad_de)\n",
        "y_pred_arg = y_pred_t_de.argmax(axis=1)\n",
        "y_pred_t_de= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "y_pred_t_fr = cnn_max_pool_mod.predict(X_test_pad_fr)\n",
        "y_pred_arg = y_pred_t_fr.argmax(axis=1)\n",
        "y_pred_t_fr= [encoder.classes_[y] for y in y_pred_arg]\n",
        "\n",
        "print('train obs.','accuracy %s' % accuracy_score(y_pred_t_de, y_test_de))\n",
        "print('train obs.','b_accuracy %s' % balanced_accuracy_score(y_pred_t_de, y_test_de))\n",
        "print('train obs.','accuracy %s' % accuracy_score(y_pred_t_fr, y_test_fr))\n",
        "print('train obs.','b_accuracy %s' % balanced_accuracy_score(y_pred_t_fr, y_test_fr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8kTGbhi9lNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYthFWd4lUTf",
        "colab_type": "text"
      },
      "source": [
        "# Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGTYK6s5nUcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install ktrain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bac0n_MGu0el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names_ = df_de['cc5'].unique()\n",
        "class_names_.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNuN8eXD6eNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run_CNN(X_train_pad_de,y_train_enc_de,X_val_pad_de,y_val_enc_de,X_test_pad_de,y_test_de,vocab_size_de,embedding_matrix_de,class_weight_dict_de,obs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrsYuXhnCTBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "var1 = 'categ'\n",
        "var2 = 'words_from_url'\n",
        "var3 = 'name'\n",
        "df_fr['text'] = 'fr ' + df_fr[var1].fillna('unknown')  + ' . ' + df_fr[var2].fillna('unknown')  + ' . ' + df_fr[var3].fillna('unknown') \n",
        "df_de['text'] = 'de ' + df_de[var1].fillna('unknown')  + ' . ' + df_de[var2].fillna('unknown')  + ' . ' + df_de[var3].fillna('unknown') \n",
        "df_at['text'] = 'de ' + df_at[var1].fillna('unknown')  + ' . ' + df_at[var2].fillna('unknown')  + ' . ' + df_at[var3].fillna('unknown') \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "def split_train_abs(df):\n",
        "    X_train, X_val_test, y_train, y_val_test  = train_test_split(df['text'], df['cc5'], random_state=42, shuffle=True)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val_test , y_val_test, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "X_train_de, X_val_de, X_test_de, y_train_de, y_val_de, y_test_de = split_train_abs(df_de)\n",
        "X_train_fr, X_val_fr, X_test_fr, y_train_fr, y_val_fr, y_test_fr = split_train_abs(df_fr)\n",
        "\n",
        "print('de',X_train_de.shape, X_val_de.shape, X_test_de.shape, y_train_de.shape, y_val_de.shape, y_test_de.shape )\n",
        "print('fr',X_train_fr.shape, X_val_fr.shape, X_test_fr.shape, y_train_fr.shape, y_val_fr.shape, y_test_fr.shape )\n",
        "\n",
        "X_train_de = [str(sen) for sen in X_train_de]\n",
        "X_val_de = [str(sen) for sen in X_val_de]\n",
        "X_test_de = [str(sen) for sen in X_test_de]\n",
        "\n",
        "y_train_de = [str(sen) for sen in y_train_de]\n",
        "y_val_de = [str(sen) for sen in y_val_de]\n",
        "y_test_de = [str(sen) for sen in y_test_de]\n",
        "\n",
        "X_train_fr = [str(sen) for sen in X_train_fr]\n",
        "X_val_fr = [str(sen) for sen in X_val_fr]\n",
        "X_test_fr = [str(sen) for sen in X_test_fr]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_de['cc5'])\n",
        "\n",
        "def encode_label(y_):\n",
        "    y__ = encoder.transform(y_)\n",
        "    #y_enc =tf.keras.utils.to_categorical(y__, num_classes=no_Classes, dtype=\"float32\")\n",
        "    return y__\n",
        "\n",
        "y_train_enc_de = encode_label(y_train_de)\n",
        "y_val_enc_de = encode_label(y_val_de)\n",
        "y_test_enc_de = encode_label(y_test_de)\n",
        "\n",
        "y_train_enc_fr = encode_label(y_train_fr)\n",
        "y_val_enc_fr = encode_label(y_val_fr)\n",
        "y_test_enc_fr = encode_label(y_test_fr)\n",
        "\n",
        "print(y_train_enc_de.shape,y_val_enc_de.shape,y_test_enc_de.shape)\n",
        "print(y_train_enc_fr.shape,y_val_enc_fr.shape,y_test_enc_fr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkAMEHNclUgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text\n",
        "MODEL_NAME = 'xlm-roberta-large'#'bert-base-multilingual-cased'\n",
        "t = text.Transformer(MODEL_NAME, maxlen=60, class_names=class_names_)\n",
        "\n",
        "trn = t.preprocess_train(X_train_de, y_train_de)\n",
        "val = t.preprocess_test(X_val_de, y_val_de)\n",
        "#test = t.preprocess_test(X_test_de, y_test_enc_de)\n",
        "model = t.get_classifier()\n",
        "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw0clG3J9VPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.lr_find()\n",
        "learner.lr_plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qmg8zlaH829",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.fit(5e-5, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLIznHW57aZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCdV1iATlUjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "from simpletransformers.classification import MultiLabelClassificationModel\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "train_data = pd.DataFrame(list(zip(X_train_de,y_train_de)), columns =['text','y'])\n",
        "eval_df = pd.DataFrame(list(zip(X_val_de,y_val_de)), columns =['text','y'])\n",
        "test_df = pd.DataFrame(list(zip(X_test_de,y_test_de)), columns =['text','y'])\n",
        "\n",
        " \n",
        "# Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int.\n",
        "#train_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]]\n",
        "#train_df = pd.DataFrame(train_data)\n",
        "\n",
        "#eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]]\n",
        "#eval_df = pd.DataFrame(eval_data)\n",
        "\n",
        "# Create a ClassificationModel\n",
        "model = ClassificationModel('XLM,', 'XLM-RoBERTa') # You can set class weights by using the optional weight argument\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_df)\n",
        "\n",
        "# Evaluate the model\n",
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwqzwcyQ3t_Z",
        "colab_type": "text"
      },
      "source": [
        "get the french"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnTHX7GJ8HVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(class_names)\n",
        "Y_FRENCH.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoCTIP6a3wgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_FRENCH = list(df_fr['text'])\n",
        "Y_FRENCH = encode_label(df_fr['cc5'],encoder5)\n",
        "\n",
        "test = t.preprocess_test(X_FRENCH, Y_FRENCH)\n",
        "learner.validate(val_data=test, class_names=t.get_classes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRp7ewNBlUmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reloaded_predictor = ktrain.get_predictor(learner.model, preproc=preproc)\n",
        "reloaded_predictor.predict(X_test[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkE07ik3xmWB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPknKZ3kmYJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1AtuawmqvKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lze_FoM3rboE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store the model we want to use\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EME7uTGWrceM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import (TFBertForSequenceClassification, \n",
        "                          BertTokenizer,\n",
        "                          TFRobertaForSequenceClassification, \n",
        "                          RobertaTokenizer)\n",
        "\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "#roberta_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
        "#roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLznIZuYtlI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence = \"Systolic arrays are cool. This 🐳 is cool too.\"\n",
        "\n",
        "bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)\n",
        "\n",
        "print(\"BERT:\", bert_tokenized_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKi3lvCmtrZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encoded_bert_sequence = bert_tokenizer.encode(list(X_train_de), add_special_tokens=True, max_length=128)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OXvSBM6v9XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.DataFrame(zip(X_train_de , y_train_de),columns=['text','cc5'])\n",
        "df_train = pd.DataFrame(zip(X_train_de , y_train_de),columns=['text','cc5'])\n",
        "df_train = pd.DataFrame(zip(X_train_de , y_train_de),columns=['text','cc5'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnxnKgVgvY7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_bert_sequence = bert_tokenizer.encode(list(X_train_de), add_special_tokens=True, max_length=64)\n",
        "\n",
        "tf_train = tf.data.Dataset.from_tensor_slices((encoded_bert_sequence, df_train['cc5'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYNnPNCpvJKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import glue_convert_examples_to_features\n",
        "\n",
        "bert_train_dataset = glue_convert_examples_to_features(tf_train, bert_tokenizer, 64, 'mrpc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT_oTop7t6H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpxrVm2qu2xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fine-tuning BERT on MRPC\")\n",
        "bert_history = bert_model.fit(bert_train_dataset, epochs=3, validation_data=bert_validation_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6FoO9u0vEZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Evaluating the BERT model\")\n",
        "bert_model.evaluate(bert_validation_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLcNppbvvErA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}